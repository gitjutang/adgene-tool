"""
Multi-Dataset Integration Framework
å¤šæ•°æ®é›†æ•´åˆæ¡†æ¶ï¼ˆADNI, ROSMAP, MSBBï¼‰

åˆ›æ–°ç‚¹ï¼š
1. è·¨æ•°æ®é›†æ ‡å‡†åŒ–å’Œæ•´åˆ
2. è·¨ç§æ—éªŒè¯
3. å…ƒåˆ†æå¢åŠ ç»Ÿè®¡åŠŸæ•ˆ
4. æ‰¹é‡æ•ˆåº”æ ¡æ­£

ä½œè€…ï¼š[Author Names]
æ—¥æœŸï¼š2026-02-03
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from scipy import stats
from scipy.stats import combine_pvalues
from typing import List, Dict, Optional
import warnings
warnings.filterwarnings('ignore')


class MultiDatasetIntegrator:
    """
    å¤šæ•°æ®é›†æ•´åˆå™¨
    
    åŠŸèƒ½ï¼š
    - è·¨æ•°æ®é›†æ ‡å‡†åŒ–
    - æ‰¹é‡æ•ˆåº”æ ¡æ­£
    - å…ƒåˆ†æ
    - è·¨ç§æ—éªŒè¯
    - ä¸€è‡´æ€§è¯„ä¼°
    """
    
    def __init__(self):
        self.datasets = {}
        self.integrated_data = None
        self.meta_analysis_results = None
        self.batch_corrected_data = None
        
    def load_dataset(self, dataset_name: str, data: pd.DataFrame, 
                   metadata: pd.DataFrame = None):
        """
        åŠ è½½æ•°æ®é›†
        
        Parameters:
        -----------
        dataset_name : str
            æ•°æ®é›†åç§° ('ADNI', 'ROSMAP', 'MSBB')
        data : pd.DataFrame
            ç»„å­¦æ•°æ®
        metadata : pd.DataFrame
            å…ƒæ•°æ®ï¼ˆç§æ—ã€å¹´é¾„ã€æ€§åˆ«ç­‰ï¼‰
        """
        self.datasets[dataset_name] = {
            'data': data,
            'metadata': metadata
        }
        print(f"âœ… åŠ è½½ {dataset_name} æ•°æ®é›†: {data.shape[0]} æ ·æœ¬ Ã— {data.shape[1]} ç‰¹å¾")
        
    def standardize_datasets(self):
        """
        æ ‡å‡†åŒ–æ‰€æœ‰æ•°æ®é›†
        
        Returns:
        --------
        Dict
            æ ‡å‡†åŒ–åçš„æ•°æ®é›†
        """
        print("\nğŸ”„ æ ‡å‡†åŒ–æ•°æ®é›†...")
        
        for dataset_name in self.datasets:
            data = self.datasets[dataset_name]['data'].copy()
            
            # Z-scoreæ ‡å‡†åŒ–
            scaler = StandardScaler()
            data_standardized = pd.DataFrame(
                scaler.fit_transform(data),
                index=data.index,
                columns=data.columns
            )
            
            self.datasets[dataset_name]['data_standardized'] = data_standardized
            print(f"   {dataset_name}: æ ‡å‡†åŒ–å®Œæˆ")
        
        return self.datasets
    
    def identify_common_features(self):
        """
        è¯†åˆ«æ‰€æœ‰æ•°æ®é›†çš„å…±åŒç‰¹å¾
        
        Returns:
        --------
        List
            å…±åŒç‰¹å¾åˆ—è¡¨
        """
        print("\nğŸ” è¯†åˆ«å…±åŒç‰¹å¾...")
        
        feature_sets = [
            set(self.datasets[ds]['data'].columns) 
            for ds in self.datasets
        ]
        
        common_features = set.intersection(*feature_sets)
        common_features = sorted(list(common_features))
        
        print(f"âœ… å…±åŒç‰¹å¾æ•°: {len(common_features)}")
        print(f"   å„æ•°æ®é›†ç‰¹å¾æ•°:")
        for ds in self.datasets:
            print(f"   - {ds}: {len(self.datasets[ds]['data'].columns)}")
        
        return common_features
    
    def correct_batch_effects(self, method: str = 'combat'):
        """
        æ‰¹é‡æ•ˆåº”æ ¡æ­£
        
        Parameters:
        -----------
        method : str
            æ ¡æ­£æ–¹æ³• ('combat', 'limma', 'mean_centering')
            
        Returns:
        --------
        Dict
            æ‰¹é‡æ•ˆåº”æ ¡æ­£åçš„æ•°æ®
        """
        print(f"\nğŸ”§ æ‰¹é‡æ•ˆåº”æ ¡æ­£ ({method})...")
        
        common_features = self.identify_common_features()
        
        if method == 'combat':
            # ç®€åŒ–çš„ComBatå®ç°
            for dataset_name in self.datasets:
                data = self.datasets[dataset_name]['data_standardized'][common_features].copy()
                
                # è®¡ç®—æ‰¹é‡æ•ˆåº”ï¼ˆæ•°æ®é›†å‡å€¼ï¼‰
                dataset_mean = data.mean()
                global_mean = pd.concat([
                    self.datasets[ds]['data_standardized'][common_features] 
                    for ds in self.datasets
                ]).mean()
                
                # æ ¡æ­£
                data_corrected = data - dataset_mean + global_mean
                
                self.datasets[dataset_name]['data_corrected'] = data_corrected
                print(f"   {dataset_name}: æ‰¹é‡æ•ˆåº”æ ¡æ­£å®Œæˆ")
                
        elif method == 'mean_centering':
            # å‡å€¼ä¸­å¿ƒåŒ–
            global_mean = pd.concat([
                self.datasets[ds]['data_standardized'][common_features] 
                for ds in self.datasets
            ]).mean()
            
            for dataset_name in self.datasets:
                data = self.datasets[dataset_name]['data_standardized'][common_features].copy()
                data_corrected = data - global_mean
                
                self.datasets[dataset_name]['data_corrected'] = data_corrected
                print(f"   {dataset_name}: å‡å€¼ä¸­å¿ƒåŒ–å®Œæˆ")
        
        self.batch_corrected_data = self.datasets
        return self.datasets
    
    def perform_meta_analysis(self, feature: str, outcome: str = 'diagnosis'):
        """
        å¯¹ç‰¹å®šç‰¹å¾è¿›è¡Œå…ƒåˆ†æ
        
        Parameters:
        -----------
        feature : str
            è¦åˆ†æçš„ç‰¹å¾
        outcome : str
            ç»“å±€å˜é‡
            
        Returns:
        --------
        Dict
            å…ƒåˆ†æç»“æœ
        """
        print(f"\nğŸ“Š å…ƒåˆ†æ: {feature}")
        
        effect_sizes = []
        variances = []
        sample_sizes = []
        
        for dataset_name in self.datasets:
            data = self.datasets[dataset_name]['data_corrected']
            metadata = self.datasets[dataset_name]['metadata']
            
            # æ£€æŸ¥ç»“å±€å˜é‡æ˜¯å¦å­˜åœ¨
            if metadata is not None and outcome in metadata.columns:
                # è®¡ç®—æ•ˆåº”é‡ï¼ˆCohen's dï¼‰
                groups = metadata[outcome].unique()
                if len(groups) == 2:
                    group1 = data[metadata[outcome] == groups[0]][feature]
                    group2 = data[metadata[outcome] == groups[1]][feature]
                    
                    # Cohen's d
                    pooled_std = np.sqrt(
                        (group1.std()**2 + group2.std()**2) / 2
                    )
                    effect_size = (group1.mean() - group2.mean()) / pooled_std
                    
                    # æ–¹å·®
                    n1, n2 = len(group1), len(group2)
                    variance = (n1 + n2) / (n1 * n2) + effect_size**2 / (2 * (n1 + n2))
                    
                    effect_sizes.append(effect_size)
                    variances.append(variance)
                    sample_sizes.append(n1 + n2)
        
        if len(effect_sizes) > 0:
            # å›ºå®šæ•ˆåº”æ¨¡å‹
            weights = [1/v for v in variances]
            pooled_effect = sum(e * w for e, w in zip(effect_sizes, weights)) / sum(weights)
            pooled_variance = 1 / sum(weights)
            pooled_se = np.sqrt(pooled_variance)
            
            # Zæ£€éªŒ
            z_score = pooled_effect / pooled_se
            p_value = 2 * (1 - stats.norm.cdf(abs(z_score)))
            
            # å¼‚è´¨æ€§æ£€éªŒï¼ˆQç»Ÿè®¡é‡ï¼‰
            q_statistic = sum(w * (e - pooled_effect)**2 
                            for e, w in zip(effect_sizes, weights))
            df = len(effect_sizes) - 1
            p_heterogeneity = 1 - stats.chi2.cdf(q_statistic, df)
            
            # IÂ²ç»Ÿè®¡é‡
            if q_statistic > df:
                i_squared = 100 * (q_statistic - df) / q_statistic
            else:
                i_squared = 0
            
            results = {
                'Feature': feature,
                'Pooled_Effect_Size': pooled_effect,
                'SE': pooled_se,
                'Z_Score': z_score,
                'P_Value': p_value,
                'Q_Statistic': q_statistic,
                'P_Heterogeneity': p_heterogeneity,
                'I_Squared': i_squared,
                'N_Studies': len(effect_sizes),
                'Effect_Sizes': effect_sizes,
                'Sample_Sizes': sample_sizes
            }
            
            print(f"   æ±‡æ€»æ•ˆåº”é‡: {pooled_effect:.3f}, P = {p_value:.3e}")
            print(f"   å¼‚è´¨æ€§: Q = {q_statistic:.2f}, IÂ² = {i_squared:.1f}%")
            
            return results
        else:
            print(f"   âš ï¸ æ— æ³•è®¡ç®—å…ƒåˆ†æï¼ˆç¼ºå°‘ç»“å±€å˜é‡ï¼‰")
            return None
    
    def perform_cross_dataset_validation(self, features: List):
        """
        è·¨æ•°æ®é›†éªŒè¯
        
        Parameters:
        -----------
        features : List
            è¦éªŒè¯çš„ç‰¹å¾åˆ—è¡¨
            
        Returns:
        --------
        pd.DataFrame
            è·¨æ•°æ®é›†éªŒè¯ç»“æœ
        """
        print(f"\nğŸ”¬ è·¨æ•°æ®é›†éªŒè¯ ({len(features)} ç‰¹å¾)...")
        
        validation_results = []
        
        for feature in features:
            results = self.perform_meta_analysis(feature)
            if results is not None:
                validation_results.append(results)
        
        self.meta_analysis_results = pd.DataFrame(validation_results)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆçš„å…ƒåˆ†æç»“æœ
        if len(self.meta_analysis_results) > 0:
            # å¤šé‡æ£€éªŒæ ¡æ­£
            from statsmodels.stats.multitest import multipletests
            p_values = self.meta_analysis_results['P_Value'].values
            _, p_corrected, _, _ = multipletests(p_values, method='fdr_bh')
            self.meta_analysis_results['P_Corrected'] = p_corrected
            self.meta_analysis_results['Significance'] = [
                '***' if p < 0.001 else '**' if p < 0.01 else 
                '*' if p < 0.05 else '' 
                for p in p_corrected
            ]
            
            print(f"âœ… è·¨æ•°æ®é›†éªŒè¯å®Œæˆ")
            print(f"   æ˜¾è‘—ç‰¹å¾æ•°: {(self.meta_analysis_results['P_Corrected'] < 0.05).sum()}")
        else:
            print("âš ï¸ æ²¡æœ‰æœ‰æ•ˆçš„å…ƒåˆ†æç»“æœ")
        
        return self.meta_analysis_results
    
    def assess_cross_ethnicity_consistency(self):
        """
        è¯„ä¼°è·¨ç§æ—ä¸€è‡´æ€§
        
        Returns:
        --------
        Dict
            è·¨ç§æ—ä¸€è‡´æ€§ç»“æœ
        """
        print("\nğŸŒ è¯„ä¼°è·¨ç§æ—ä¸€è‡´æ€§...")
        
        consistency_results = {}
        
        # æ£€æŸ¥å…ƒæ•°æ®ä¸­çš„ç§æ—ä¿¡æ¯
        for dataset_name in self.datasets:
            metadata = self.datasets[dataset_name]['metadata']
            if 'Race' in metadata.columns or 'Ethnicity' in metadata.columns:
                race_col = 'Race' if 'Race' in metadata.columns else 'Ethnicity'
                races = metadata[race_col].unique()
                print(f"   {dataset_name}: {len(races)} ä¸ªç§æ—")
        
        # è®¡ç®—è·¨ç§æ—ç›¸å…³æ€§
        common_features = self.identify_common_features()
        
        for i, ds1 in enumerate(list(self.datasets.keys())):
            for j, ds2 in enumerate(list(self.datasets.keys())):
                if i < j:
                    data1 = self.datasets[ds1]['data_corrected'][common_features]
                    data2 = self.datasets[ds2]['data_corrected'][common_features]
                    
                    # è®¡ç®—ç›¸å…³æ€§ï¼ˆä½¿ç”¨å…±åŒæ ·æœ¬æˆ–å…¨éƒ¨æ ·æœ¬çš„å¹³å‡å€¼ï¼‰
                    correlations = []
                    for feat in common_features:
                        # ä½¿ç”¨ç‰¹å¾çš„å¹³å‡å€¼è¿›è¡Œè·¨æ•°æ®é›†æ¯”è¾ƒ
                        mean1 = data1[feat].mean()
                        mean2 = data2[feat].mean()
                        
                        # è®¡ç®—è·¨æ•°æ®é›†çš„å˜å¼‚ç³»æ•°
                        cv1 = data1[feat].std() / abs(mean1) if mean1 != 0 else 0
                        cv2 = data2[feat].std() / abs(mean2) if mean2 != 0 else 0
                        
                        # ä½¿ç”¨ä¸€è‡´æ€§æŒ‡æ ‡ï¼ˆ1 - |CV1 - CV2|ï¼‰
                        consistency = 1 - abs(cv1 - cv2)
                        correlations.append(consistency)
                    
                    consistency_results[f'{ds1}_vs_{ds2}'] = {
                        'Mean_Correlation': np.mean(correlations),
                        'Median_Correlation': np.median(correlations),
                        'SD_Correlation': np.std(correlations),
                        'N_Features': len(common_features)
                    }
        
        print("âœ… è·¨ç§æ—ä¸€è‡´æ€§è¯„ä¼°å®Œæˆ")
        return consistency_results
    
    def integrate_datasets(self):
        """
        æ•´åˆæ‰€æœ‰æ•°æ®é›†
        
        Returns:
        --------
        pd.DataFrame
            æ•´åˆåçš„æ•°æ®
        """
        print("\nğŸ”— æ•´åˆæ•°æ®é›†...")
        
        common_features = self.identify_common_features()
        
        integrated_list = []
        for dataset_name in self.datasets:
            data = self.datasets[dataset_name]['data_corrected'][common_features].copy()
            data['Dataset'] = dataset_name
            
            if self.datasets[dataset_name]['metadata'] is not None:
                metadata = self.datasets[dataset_name]['metadata']
                for col in metadata.columns:
                    if col not in data.columns:
                        data[col] = metadata[col].values
            
            integrated_list.append(data)
        
        self.integrated_data = pd.concat(integrated_list, axis=0, ignore_index=True)
        
        print(f"âœ… æ•°æ®é›†æ•´åˆå®Œæˆ:")
        print(f"   æ€»æ ·æœ¬æ•°: {len(self.integrated_data)}")
        print(f"   ç‰¹å¾æ•°: {len(common_features)}")
        print(f"   æ•°æ®é›†æ•°: {len(self.datasets)}")
        
        return self.integrated_data
    
    def visualize_multi_dataset_results(self, output_dir: str = './results'):
        """
        å¯è§†åŒ–å¤šæ•°æ®é›†æ•´åˆç»“æœ
        
        Parameters:
        -----------
        output_dir : str
            è¾“å‡ºç›®å½•
        """
        import os
        os.makedirs(output_dir, exist_ok=True)
        
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        
        # 1. æ•°æ®é›†æ ·æœ¬é‡æ¯”è¾ƒ
        sample_sizes = [len(self.datasets[ds]['data']) for ds in self.datasets]
        dataset_names = list(self.datasets.keys())
        
        axes[0, 0].bar(range(len(dataset_names)), sample_sizes, 
                         color=['steelblue', 'coral', 'lightgreen'][:len(dataset_names)])
        axes[0, 0].set_xticks(range(len(dataset_names)))
        axes[0, 0].set_xticklabels(dataset_names)
        axes[0, 0].set_ylabel('Sample Size')
        axes[0, 0].set_title('Sample Size by Dataset')
        axes[0, 0].grid(True, alpha=0.3, axis='y')
        
        # 2. PCAå¯è§†åŒ–ï¼ˆæ•´åˆæ•°æ®ï¼‰
        if self.integrated_data is not None:
            numeric_cols = self.integrated_data.select_dtypes(include=[np.number]).columns
            numeric_cols = [col for col in numeric_cols if col != 'Dataset']
            
            pca = PCA(n_components=2)
            pca_result = pca.fit_transform(
                self.integrated_data[numeric_cols].fillna(0)
            )
            
            for i, dataset in enumerate(self.integrated_data['Dataset'].unique()):
                mask = self.integrated_data['Dataset'] == dataset
                axes[0, 1].scatter(pca_result[mask, 0], pca_result[mask, 1], 
                                   label=dataset, alpha=0.6)
            
            axes[0, 1].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%})')
            axes[0, 1].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%})')
            axes[0, 1].set_title('PCA of Integrated Data')
            axes[0, 1].legend()
            axes[0, 1].grid(True, alpha=0.3)
        
        # 3. å…ƒåˆ†ææ•ˆåº”é‡æ£®æ—å›¾
        if self.meta_analysis_results is not None and len(self.meta_analysis_results) > 0:
            top_features = self.meta_analysis_results.nsmallest(10, 'P_Corrected')
            
            y_pos = range(len(top_features))
            axes[0, 2].barh(y_pos, top_features['Pooled_Effect_Size'], 
                             xerr=top_features['SE'], color='coral', alpha=0.7)
            axes[0, 2].set_yticks(y_pos)
            axes[0, 2].set_yticklabels(top_features['Feature'], fontsize=8)
            axes[0, 2].set_xlabel('Pooled Effect Size')
            axes[0, 2].set_title('Meta-Analysis: Top 10 Features')
            axes[0, 2].axvline(x=0, color='black', linestyle='--', linewidth=1)
            axes[0, 2].grid(True, alpha=0.3, axis='x')
        else:
            axes[0, 2].text(0.5, 0.5, 'No meta-analysis results', 
                           ha='center', va='center', transform=axes[0, 2].transAxes)
            axes[0, 2].set_title('Meta-Analysis')
        
        # 4. å¼‚è´¨æ€§åˆ†æ
        if self.meta_analysis_results is not None and len(self.meta_analysis_results) > 0:
            axes[1, 0].scatter(self.meta_analysis_results['Pooled_Effect_Size'], 
                               self.meta_analysis_results['I_Squared'], 
                               alpha=0.6)
            axes[1, 0].set_xlabel('Pooled Effect Size')
            axes[1, 0].set_ylabel('IÂ² (%)')
            axes[1, 0].set_title('Heterogeneity Analysis')
            axes[1, 0].grid(True, alpha=0.3)
        else:
            axes[1, 0].text(0.5, 0.5, 'No meta-analysis results', 
                           ha='center', va='center', transform=axes[1, 0].transAxes)
            axes[1, 0].set_title('Heterogeneity Analysis')
        
        # 5. På€¼åˆ†å¸ƒ
        if self.meta_analysis_results is not None and len(self.meta_analysis_results) > 0:
            axes[1, 1].hist(self.meta_analysis_results['P_Corrected'], 
                            bins=30, color='lightgreen', alpha=0.7, edgecolor='black')
            axes[1, 1].axvline(0.05, color='red', linestyle='--', 
                                 linewidth=2, label='Î± = 0.05')
            axes[1, 1].set_xlabel('Corrected P-Value')
            axes[1, 1].set_ylabel('Frequency')
            axes[1, 1].set_title('P-Value Distribution')
            axes[1, 1].legend()
            axes[1, 1].grid(True, alpha=0.3)
        else:
            axes[1, 1].text(0.5, 0.5, 'No meta-analysis results', 
                           ha='center', va='center', transform=axes[1, 1].transAxes)
            axes[1, 1].set_title('P-Value Distribution')
        
        # 6. æ˜¾è‘—ç‰¹å¾æ•°é‡
        if self.meta_analysis_results is not None and len(self.meta_analysis_results) > 0:
            significance_levels = ['***', '**', '*', '']
            sig_counts = [
                (self.meta_analysis_results['Significance'] == level).sum() 
                for level in significance_levels
            ]
            
            axes[1, 2].bar(range(len(significance_levels)), sig_counts, 
                             color=['red', 'orange', 'yellow', 'lightgray'])
            axes[1, 2].set_xticks(range(len(significance_levels)))
            axes[1, 2].set_xticklabels(significance_levels)
            axes[1, 2].set_ylabel('Count')
            axes[1, 2].set_title('Significant Features')
            axes[1, 2].grid(True, alpha=0.3, axis='y')
        else:
            axes[1, 2].text(0.5, 0.5, 'No meta-analysis results', 
                           ha='center', va='center', transform=axes[1, 2].transAxes)
            axes[1, 2].set_title('Significant Features')
        
        plt.tight_layout()
        plt.savefig(f'{output_dir}/multi_dataset_integration.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… å¤šæ•°æ®é›†æ•´åˆå¯è§†åŒ–ä¿å­˜åˆ°: {output_dir}/multi_dataset_integration.png")


def demo_multi_dataset_integration():
    """
    æ¼”ç¤ºå¤šæ•°æ®é›†æ•´åˆ
    """
    print("=" * 60)
    print("å¤šæ•°æ®é›†æ•´åˆæ¡†æ¶æ¼”ç¤º")
    print("=" * 60)
    
    # åˆ›å»ºæ•´åˆå™¨
    integrator = MultiDatasetIntegrator()
    
    # æ¨¡æ‹ŸADNIæ•°æ®
    np.random.seed(42)
    n_adni = 300
    adni_data = pd.DataFrame(
        np.random.randn(n_adni, 50),
        index=[f'ADNI_{i}' for i in range(n_adni)],
        columns=[f'Gene_{i}' for i in range(50)]
    )
    adni_metadata = pd.DataFrame({
        'Diagnosis': np.random.choice(['CN', 'AD'], n_adni, p=[0.6, 0.4]),
        'Age': np.random.normal(75, 8, n_adni),
        'Sex': np.random.choice(['M', 'F'], n_adni),
        'Race': np.random.choice(['White', 'Black', 'Asian'], n_adni)
    }, index=[f'ADNI_{i}' for i in range(n_adni)])
    
    # æ¨¡æ‹ŸROSMAPæ•°æ®
    n_rosmap = 250
    rosmap_data = pd.DataFrame(
        np.random.randn(n_rosmap, 50),
        index=[f'ROSMAP_{i}' for i in range(n_rosmap)],
        columns=[f'Gene_{i}' for i in range(50)]
    )
    rosmap_metadata = pd.DataFrame({
        'Diagnosis': np.random.choice(['CN', 'AD'], n_rosmap, p=[0.55, 0.45]),
        'Age': np.random.normal(80, 10, n_rosmap),
        'Sex': np.random.choice(['M', 'F'], n_rosmap),
        'Race': np.random.choice(['White', 'Black'], n_rosmap)
    }, index=[f'ROSMAP_{i}' for i in range(n_rosmap)])
    
    # æ¨¡æ‹ŸMSBBæ•°æ®
    n_msbb = 200
    msbb_data = pd.DataFrame(
        np.random.randn(n_msbb, 50),
        index=[f'MSBB_{i}' for i in range(n_msbb)],
        columns=[f'Gene_{i}' for i in range(50)]
    )
    msbb_metadata = pd.DataFrame({
        'Diagnosis': np.random.choice(['CN', 'AD'], n_msbb, p=[0.5, 0.5]),
        'Age': np.random.normal(85, 12, n_msbb),
        'Sex': np.random.choice(['M', 'F'], n_msbb),
        'Race': np.random.choice(['White', 'Black', 'Hispanic'], n_msbb)
    }, index=[f'MSBB_{i}' for i in range(n_msbb)])
    
    # åŠ è½½æ•°æ®é›†
    integrator.load_dataset('ADNI', adni_data, adni_metadata)
    integrator.load_dataset('ROSMAP', rosmap_data, rosmap_metadata)
    integrator.load_dataset('MSBB', msbb_data, msbb_metadata)
    
    # æ ‡å‡†åŒ–æ•°æ®é›†
    integrator.standardize_datasets()
    
    # æ‰¹é‡æ•ˆåº”æ ¡æ­£
    integrator.correct_batch_effects(method='combat')
    
    # è·¨æ•°æ®é›†éªŒè¯
    features_to_validate = [f'Gene_{i}' for i in range(10)]
    validation_results = integrator.perform_cross_dataset_validation(features_to_validate)
    
    if validation_results is not None:
        print("\nğŸ“Š å…ƒåˆ†æç»“æœï¼ˆå‰5ä¸ªç‰¹å¾ï¼‰:")
        print(validation_results.head())
    
    # è·¨ç§æ—ä¸€è‡´æ€§
    consistency = integrator.assess_cross_ethnicity_consistency()
    
    # æ•´åˆæ•°æ®é›†
    integrated_data = integrator.integrate_datasets()
    
    # å¯è§†åŒ–ç»“æœ
    integrator.visualize_multi_dataset_results(output_dir='./results/multi_dataset_integration')
    
    print("\n" + "=" * 60)
    print("âœ… å¤šæ•°æ®é›†æ•´åˆæ¡†æ¶æ¼”ç¤ºå®Œæˆï¼")
    print("=" * 60)


if __name__ == '__main__':
    demo_multi_dataset_integration()