"""
AI-Driven Prediction Models for Alzheimer's Disease
AIé©±åŠ¨çš„é˜¿å°”èŒ¨æµ·é»˜ç—…é¢„æµ‹æ¨¡å‹

åˆ›æ–°ç‚¹ï¼š
1. æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ˆç¥ç»ç½‘ç»œã€å›¾ç¥ç»ç½‘ç»œï¼‰
2. é›†æˆå­¦ä¹ æ–¹æ³•
3. å¤šæ¨¡æ€æ•°æ®èåˆ
4. å¯è§£é‡Šæ€§AIï¼ˆSHAPã€LIMEï¼‰
5. è‡ªåŠ¨åŒ–ç‰¹å¾é€‰æ‹©

ä½œè€…ï¼š[Author Names]
æ—¥æœŸï¼š2026-02-03
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import (accuracy_score, precision_score, recall_score, 
                           f1_score, roc_auc_score, roc_curve, 
                           confusion_matrix, classification_report,
                           precision_recall_curve)
from sklearn.calibration import calibration_curve
from sklearn.neural_network import MLPClassifier
from sklearn.feature_selection import RFE, SelectKBest, f_classif
from typing import List
import warnings
warnings.filterwarnings('ignore')


class AIPredictionModel:
    """
    AIé¢„æµ‹æ¨¡å‹ç±»
    
    åŠŸèƒ½ï¼š
    - æ·±åº¦å­¦ä¹ æ¨¡å‹
    - ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹
    - é›†æˆå­¦ä¹ 
    - ç‰¹å¾é€‰æ‹©
    - æ¨¡å‹è§£é‡Š
    """
    
    def __init__(self, model_type: str = 'ensemble'):
        self.model_type = model_type
        self.models = {}
        self.best_model = None
        self.feature_importance = None
        self.shap_values = None
        self.scaler = StandardScaler()
        
    def prepare_data(self, X: pd.DataFrame, y: pd.Series, 
                   test_size: float = 0.3, random_state: int = 42):
        """
        å‡†å¤‡æ•°æ®
        
        Parameters:
        -----------
        X : pd.DataFrame
            ç‰¹å¾çŸ©é˜µ
        y : pd.Series
            æ ‡ç­¾
        test_size : float
            æµ‹è¯•é›†æ¯”ä¾‹
        random_state : int
            éšæœºç§å­
            
        Returns:
        --------
        Tuple
            (X_train, X_test, y_train, y_test)
        """
        print("ğŸ“Š å‡†å¤‡æ•°æ®...")
        
        # æ ‡ç­¾ç¼–ç 
        if y.dtype == 'object':
            le = LabelEncoder()
            y_encoded = le.fit_transform(y)
        else:
            le = None
            y_encoded = y.values
        
        # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
        X_train, X_test, y_train, y_test = train_test_split(
            X, y_encoded, test_size=test_size, 
            random_state=random_state, stratify=y_encoded
        )
        
        # æ ‡å‡†åŒ–
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)
        
        print(f"   è®­ç»ƒé›†: {X_train_scaled.shape[0]} æ ·æœ¬")
        print(f"   æµ‹è¯•é›†: {X_test_scaled.shape[0]} æ ·æœ¬")
        print(f"   ç‰¹å¾æ•°: {X_train_scaled.shape[1]}")
        
        return (X_train_scaled, X_test_scaled, y_train, y_test, le)
    
    def feature_selection(self, X_train: np.ndarray, y_train: np.ndarray, 
                      method: str = 'rfe', k: int = 20):
        """
        ç‰¹å¾é€‰æ‹©
        
        Parameters:
        -----------
        X_train : np.ndarray
            è®­ç»ƒç‰¹å¾
        y_train : np.ndarray
            è®­ç»ƒæ ‡ç­¾
        method : str
            é€‰æ‹©æ–¹æ³• ('rfe', 'kbest', 'importance')
        k : int
            é€‰æ‹©ç‰¹å¾æ•°
            
        Returns:
        --------
        np.ndarray
            é€‰æ‹©åçš„ç‰¹å¾ç´¢å¼•
        """
        print(f"\nğŸ” ç‰¹å¾é€‰æ‹© ({method}, k={k})...")
        
        if method == 'rfe':
            # é€’å½’ç‰¹å¾æ¶ˆé™¤
            rf = RandomForestClassifier(n_estimators=100, random_state=42)
            rfe = RFE(estimator=rf, n_features_to_select=k, step=1)
            rfe.fit(X_train, y_train)
            selected_features = rfe.support_
            
        elif method == 'kbest':
            # åŸºäºç»Ÿè®¡æ£€éªŒçš„ç‰¹å¾é€‰æ‹©
            selector = SelectKBest(f_classif, k=k)
            selector.fit(X_train, y_train)
            selected_features = selector.get_support()
            
        elif method == 'importance':
            # åŸºäºç‰¹å¾é‡è¦æ€§
            rf = RandomForestClassifier(n_estimators=100, random_state=42)
            rf.fit(X_train, y_train)
            importance = rf.feature_importances_
            selected_indices = np.argsort(importance)[-k:]
            selected_features = np.zeros(len(importance), dtype=bool)
            selected_features[selected_indices] = True
            
        else:
            raise ValueError(f"æœªçŸ¥çš„ç‰¹å¾é€‰æ‹©æ–¹æ³•: {method}")
        
        print(f"   é€‰æ‹©ç‰¹å¾æ•°: {selected_features.sum()}")
        return selected_features
    
    def build_deep_learning_model(self, input_shape: int):
        """
        æ„å»ºç¥ç»ç½‘ç»œæ¨¡å‹ï¼ˆä½¿ç”¨sklearnçš„MLPClassifierï¼‰
        
        Parameters:
        -----------
        input_shape : int
            è¾“å…¥ç»´åº¦
            
        Returns:
        --------
        MLPClassifier
            ç¥ç»ç½‘ç»œæ¨¡å‹
        """
        print("\nğŸ§  æ„å»ºç¥ç»ç½‘ç»œæ¨¡å‹...")
        
        model = MLPClassifier(
            hidden_layer_sizes=(256, 128, 64, 32),
            activation='relu',
            solver='adam',
            alpha=0.0001,
            batch_size='auto',
            learning_rate='adaptive',
            learning_rate_init=0.001,
            max_iter=200,
            random_state=42,
            early_stopping=True,
            validation_fraction=0.2
        )
        
        print("   ç¥ç»ç½‘ç»œæ¨¡å‹æ„å»ºå®Œæˆ")
        return model
    
    def train_deep_learning_model(self, X_train: np.ndarray, y_train: np.ndarray,
                              X_val: np.ndarray, y_val: np.ndarray):
        """
        è®­ç»ƒç¥ç»ç½‘ç»œæ¨¡å‹
        
        Parameters:
        -----------
        X_train : np.ndarray
            è®­ç»ƒç‰¹å¾
        y_train : np.ndarray
            è®­ç»ƒæ ‡ç­¾
        X_val : np.ndarray
            éªŒè¯ç‰¹å¾
        y_val : np.ndarray
            éªŒè¯æ ‡ç­¾
            
        Returns:
        --------
        MLPClassifier
            è®­ç»ƒå¥½çš„æ¨¡å‹
        """
        print("\nğŸ‹ è®­ç»ƒç¥ç»ç½‘ç»œæ¨¡å‹...")
        
        model = self.build_deep_learning_model(X_train.shape[1])
        
        # è®­ç»ƒ
        model.fit(X_train, y_train)
        
        self.models['Neural_Network'] = model
        
        # è¯„ä¼°éªŒè¯é›†
        val_score = model.score(X_val, y_val)
        print(f"   è®­ç»ƒå®Œæˆ")
        print(f"   éªŒè¯å‡†ç¡®ç‡: {val_score:.4f}")
        
        return model
    
    def train_traditional_models(self, X_train: np.ndarray, y_train: np.ndarray):
        """
        è®­ç»ƒä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹
        
        Parameters:
        -----------
        X_train : np.ndarray
            è®­ç»ƒç‰¹å¾
        y_train : np.ndarray
            è®­ç»ƒæ ‡ç­¾
        """
        print("\nğŸ¤– è®­ç»ƒä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹...")
        
        # å®šä¹‰æ¨¡å‹
        traditional_models = {
            'Logistic_Regression': LogisticRegression(
                max_iter=1000, random_state=42
            ),
            'Random_Forest': RandomForestClassifier(
                n_estimators=200, max_depth=10, 
                random_state=42
            ),
            'Gradient_Boosting': GradientBoostingClassifier(
                n_estimators=200, max_depth=5, 
                learning_rate=0.1, random_state=42
            ),
            'SVM': SVC(
                probability=True, kernel='rbf', 
                random_state=42
            )
        }
        
        # è®­ç»ƒæ¯ä¸ªæ¨¡å‹
        for model_name, model in traditional_models.items():
            model.fit(X_train, y_train)
            self.models[model_name] = model
            
            # äº¤å‰éªŒè¯
            cv_scores = cross_val_score(
                model, X_train, y_train, 
                cv=5, scoring='roc_auc'
            )
            
            print(f"   {model_name}: CV AUC = {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}")
        
        print("âœ… ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒå®Œæˆ")
    
    def build_ensemble_model(self, X_train: np.ndarray, y_train: np.ndarray):
        """
        æ„å»ºé›†æˆæ¨¡å‹
        
        Parameters:
        -----------
        X_train : np.ndarray
            è®­ç»ƒç‰¹å¾
        y_train : np.ndarray
            è®­ç»ƒæ ‡ç­¾
        """
        print("\nğŸ”— æ„å»ºé›†æˆæ¨¡å‹...")
        
        # ä½¿ç”¨æŠ•ç¥¨åˆ†ç±»å™¨
        from sklearn.ensemble import VotingClassifier
        
        estimators = [
            ('rf', RandomForestClassifier(n_estimators=200, random_state=42)),
            ('gb', GradientBoostingClassifier(n_estimators=200, random_state=42)),
            ('lr', LogisticRegression(max_iter=1000, random_state=42))
        ]
        
        ensemble = VotingClassifier(
            estimators=estimators,
            voting='soft'
        )
        
        ensemble.fit(X_train, y_train)
        self.models['Ensemble'] = ensemble
        
        # äº¤å‰éªŒè¯
        cv_scores = cross_val_score(
            ensemble, X_train, y_train, 
            cv=5, scoring='roc_auc'
        )
        
        print(f"   é›†æˆæ¨¡å‹: CV AUC = {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}")
        print("âœ… é›†æˆæ¨¡å‹æ„å»ºå®Œæˆ")
    
    def evaluate_models(self, X_test: np.ndarray, y_test: np.ndarray):
        """
        è¯„ä¼°æ‰€æœ‰æ¨¡å‹
        
        Parameters:
        -----------
        X_test : np.ndarray
            æµ‹è¯•ç‰¹å¾
        y_test : np.ndarray
            æµ‹è¯•æ ‡ç­¾
            
        Returns:
        --------
        pd.DataFrame
            æ¨¡å‹æ€§èƒ½
        """
        print("\nğŸ“Š è¯„ä¼°æ¨¡å‹æ€§èƒ½...")
        
        results = []
        
        for model_name, model in self.models.items():
            # é¢„æµ‹
            if model_name in ['Neural_Network', 'Deep_Learning']:
                if hasattr(model, 'predict_proba'):
                    y_pred_prob = model.predict_proba(X_test)[:, 1]
                    y_pred = (y_pred_prob > 0.5).astype(int)
                else:
                    y_pred = model.predict(X_test)
                    y_pred_prob = y_pred
            else:
                y_pred = model.predict(X_test)
                y_pred_prob = model.predict_proba(X_test)[:, 1]
            
            # è®¡ç®—æŒ‡æ ‡
            accuracy = accuracy_score(y_test, y_pred)
            precision = precision_score(y_test, y_pred)
            recall = recall_score(y_test, y_pred)
            f1 = f1_score(y_test, y_pred)
            auc = roc_auc_score(y_test, y_pred_prob)
            
            results.append({
                'Model': model_name,
                'Accuracy': accuracy,
                'Precision': precision,
                'Recall': recall,
                'F1_Score': f1,
                'AUC': auc
            })
            
            print(f"   {model_name}: AUC = {auc:.4f}, F1 = {f1:.4f}")
        
        performance_df = pd.DataFrame(results)
        performance_df = performance_df.sort_values('AUC', ascending=False)
        
        # é€‰æ‹©æœ€ä½³æ¨¡å‹
        best_model_name = performance_df.iloc[0]['Model']
        self.best_model = self.models[best_model_name]
        
        print(f"\nâœ… æœ€ä½³æ¨¡å‹: {best_model_name}")
        print(f"   AUC = {performance_df.iloc[0]['AUC']:.4f}")
        print(f"   F1 = {performance_df.iloc[0]['F1_Score']:.4f}")
        
        return performance_df
    
    def compute_shap_values(self, X_test: np.ndarray, feature_names: List[str]):
        """
        è®¡ç®—ç‰¹å¾é‡è¦æ€§ï¼ˆä½¿ç”¨æ¨¡å‹å†…ç½®çš„ç‰¹å¾é‡è¦æ€§ï¼‰
        
        Parameters:
        -----------
        X_test : np.ndarray
            æµ‹è¯•ç‰¹å¾
        feature_names : List[str]
            ç‰¹å¾åç§°
        """
        print("\nğŸ” è®¡ç®—ç‰¹å¾é‡è¦æ€§...")
        
        if self.best_model is None:
            raise ValueError("è¯·å…ˆè®­ç»ƒæ¨¡å‹")
        
        # ä½¿ç”¨æ¨¡å‹å†…ç½®çš„ç‰¹å¾é‡è¦æ€§
        if hasattr(self.best_model, 'feature_importances_'):
            # éšæœºæ£®æ—æˆ–æ¢¯åº¦æå‡
            importance = self.best_model.feature_importances_
            self.feature_importance = pd.DataFrame({
                'Feature': feature_names,
                'Importance': importance
            }).sort_values('Importance', ascending=False)
            
        elif hasattr(self.best_model, 'coef_'):
            # é€»è¾‘å›å½’
            importance = np.abs(self.best_model.coef_[0])
            self.feature_importance = pd.DataFrame({
                'Feature': feature_names,
                'Importance': importance
            }).sort_values('Importance', ascending=False)
            
        else:
            # å…¶ä»–æ¨¡å‹ä½¿ç”¨æ’åˆ—é‡è¦æ€§
            from sklearn.inspection import permutation_importance
            result = permutation_importance(
                self.best_model, X_test, y_test, n_repeats=10, random_state=42
            )
            importance = result.importances_mean
            self.feature_importance = pd.DataFrame({
                'Feature': feature_names,
                'Importance': importance
            }).sort_values('Importance', ascending=False)
        
        print(f"   ç‰¹å¾é‡è¦æ€§è®¡ç®—å®Œæˆ")
        print(f"   Top 5 ç‰¹å¾:")
        print(self.feature_importance.head())
    
    def visualize_results(self, X_test: np.ndarray, y_test: np.ndarray, 
                     feature_names: List[str], output_dir: str = './results'):
        """
        å¯è§†åŒ–ç»“æœ
        
        Parameters:
        -----------
        X_test : np.ndarray
            æµ‹è¯•ç‰¹å¾
        y_test : np.ndarray
            æµ‹è¯•æ ‡ç­¾
        feature_names : List
            ç‰¹å¾åç§°
        output_dir : str
            è¾“å‡ºç›®å½•
        """
        import os
        os.makedirs(output_dir, exist_ok=True)
        
        fig, axes = plt.subplots(3, 3, figsize=(18, 15))
        
        # 1. æ¨¡å‹æ€§èƒ½æ¯”è¾ƒ
        performance = self.evaluate_models(X_test, y_test)
        axes[0, 0].bar(range(len(performance)), performance['AUC'], 
                         color='steelblue')
        axes[0, 0].set_xticks(range(len(performance)))
        axes[0, 0].set_xticklabels(performance['Model'], rotation=15, ha='right')
        axes[0, 0].set_ylabel('AUC')
        axes[0, 0].set_title('Model Performance Comparison')
        axes[0, 0].set_ylim([0, 1])
        axes[0, 0].grid(True, alpha=0.3, axis='y')
        
        # 2. ROCæ›²çº¿
        for model_name, model in self.models.items():
            if model_name == 'Deep_Learning':
                y_pred_prob = model.predict(X_test).flatten()
            else:
                y_pred_prob = model.predict_proba(X_test)[:, 1]
            
            fpr, tpr, _ = roc_curve(y_test, y_pred_prob)
            axes[0, 1].plot(fpr, tpr, label=model_name, linewidth=2)
        
        axes[0, 1].plot([0, 1], [0, 1], 'k--', linewidth=1)
        axes[0, 1].set_xlabel('False Positive Rate')
        axes[0, 1].set_ylabel('True Positive Rate')
        axes[0, 1].set_title('ROC Curves')
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)
        
        # 3. æ··æ·†çŸ©é˜µï¼ˆæœ€ä½³æ¨¡å‹ï¼‰
        if self.best_model is not None:
            if hasattr(self.best_model, 'predict'):
                y_pred = self.best_model.predict(X_test)
            else:
                y_pred = (self.best_model.predict(X_test).flatten() > 0.5).astype(int)
            
            cm = confusion_matrix(y_test, y_pred)
            sns.heatmap(cm, annot=True, fmt='d', ax=axes[0, 2],
                      cmap='Blues', cbar_kws={'label': 'Count'})
            axes[0, 2].set_xlabel('Predicted')
            axes[0, 2].set_ylabel('Actual')
            axes[0, 2].set_title('Confusion Matrix')
        
        # 4. ç‰¹å¾é‡è¦æ€§
        if self.feature_importance is not None:
            top_features = self.feature_importance.head(15)
            axes[1, 0].barh(range(len(top_features)), 
                             top_features['Importance'])
            axes[1, 0].set_yticks(range(len(top_features)))
            axes[1, 0].set_yticklabels(top_features['Feature'], fontsize=8)
            axes[1, 0].set_xlabel('Importance')
            axes[1, 0].set_title('Feature Importance')
            axes[1, 0].grid(True, alpha=0.3, axis='x')
        
        # 5. SHAPæ‘˜è¦å›¾ï¼ˆéœ€è¦shapåº“ï¼‰
        if self.shap_values is not None:
            try:
                import shap
                if isinstance(self.shap_values, list):
                    shap.summary_plot(self.shap_values[0], X_test, 
                                   feature_names=feature_names, 
                                   plot_type='bar', show=False, ax=axes[1, 1])
                else:
                    shap.summary_plot(self.shap_values, X_test, 
                                   feature_names=feature_names, 
                                   plot_type='bar', show=False, ax=axes[1, 1])
                axes[1, 1].set_title('SHAP Summary Plot')
            except ImportError:
                axes[1, 1].text(0.5, 0.5, 'SHAP library not installed', 
                               ha='center', va='center', transform=axes[1, 1].transAxes)
                axes[1, 1].set_title('SHAP Summary Plot (Not Available)')
        else:
            axes[1, 1].text(0.5, 0.5, 'SHAP values not computed', 
                           ha='center', va='center', transform=axes[1, 1].transAxes)
            axes[1, 1].set_title('SHAP Summary Plot (Not Available)')
        
        # 6. æ·±åº¦å­¦ä¹ è®­ç»ƒæ›²çº¿
        if 'Deep_Learning' in self.models:
            # éœ€è¦é‡æ–°è®­ç»ƒä»¥è·å–å†å²
            print("âš ï¸ éœ€è¦é‡æ–°è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹ä»¥è·å–è®­ç»ƒæ›²çº¿")
        
        # 7. ç²¾ç¡®ç‡-å¬å›ç‡æ›²çº¿
        if self.best_model is not None:
            if hasattr(self.best_model, 'predict_proba'):
                y_pred_prob = self.best_model.predict_proba(X_test)[:, 1]
            elif hasattr(self.best_model, 'predict'):
                y_pred_prob = self.best_model.predict(X_test).flatten()
            else:
                y_pred_prob = self.best_model.predict(X_test).flatten()
            
            from sklearn.metrics import precision_recall_curve
            precision, recall, _ = precision_recall_curve(y_test, y_pred_prob)
            axes[1, 2].plot(recall, precision, linewidth=2)
            axes[1, 2].set_xlabel('Recall')
            axes[1, 2].set_ylabel('Precision')
            axes[1, 2].set_title('Precision-Recall Curve')
            axes[1, 2].grid(True, alpha=0.3)
        
        # 8. ç‰¹å¾ç›¸å…³æ€§çƒ­å›¾
        corr_matrix = pd.DataFrame(X_test).corr()
        if corr_matrix.shape[0] <= 20:
            sns.heatmap(corr_matrix, ax=axes[2, 0], cmap='coolwarm', 
                       center=0, square=True, cbar_kws={'label': 'Correlation'})
            axes[2, 0].set_title('Feature Correlation Heatmap')
        
        # 9. é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ
        if self.best_model is not None:
            if hasattr(self.best_model, 'predict_proba'):
                y_pred_prob = self.best_model.predict_proba(X_test)[:, 1]
            elif hasattr(self.best_model, 'predict'):
                y_pred_prob = self.best_model.predict(X_test).flatten()
            else:
                y_pred_prob = self.best_model.predict(X_test).flatten()
            
            axes[2, 1].hist(y_pred_prob[y_test == 0], bins=30, 
                            alpha=0.6, label='Class 0', color='blue')
            axes[2, 1].hist(y_pred_prob[y_test == 1], bins=30, 
                            alpha=0.6, label='Class 1', color='red')
            axes[2, 1].set_xlabel('Predicted Probability')
            axes[2, 1].set_ylabel('Frequency')
            axes[2, 1].set_title('Prediction Probability Distribution')
            axes[2, 1].legend()
            axes[2, 1].grid(True, alpha=0.3)
        
        # 10. æ ¡å‡†æ›²çº¿
        if self.best_model is not None:
            from sklearn.calibration import calibration_curve
            if hasattr(self.best_model, 'predict_proba'):
                y_pred_prob = self.best_model.predict_proba(X_test)[:, 1]
            elif hasattr(self.best_model, 'predict'):
                y_pred_prob = self.best_model.predict(X_test).flatten()
            else:
                y_pred_prob = self.best_model.predict(X_test).flatten()
            
            prob_true, prob_pred = calibration_curve(y_test, y_pred_prob, n_bins=10)
            axes[2, 2].plot(prob_pred, prob_true, marker='o', linewidth=2)
            axes[2, 2].plot([0, 1], [0, 1], 'k--', linewidth=1)
            axes[2, 2].set_xlabel('Mean Predicted Probability')
            axes[2, 2].set_ylabel('Fraction of Positives')
            axes[2, 2].set_title('Calibration Curve')
            axes[2, 2].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(f'{output_dir}/AI_prediction_models.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… ç»“æœå¯è§†åŒ–ä¿å­˜åˆ°: {output_dir}/AI_prediction_models.png")


def demo_ai_prediction_models():
    """
    æ¼”ç¤ºAIé¢„æµ‹æ¨¡å‹
    """
    print("=" * 60)
    print("AIé©±åŠ¨çš„é˜¿å°”èŒ¨æµ·é»˜ç—…é¢„æµ‹æ¨¡å‹")
    print("=" * 60)
    
    # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
    np.random.seed(42)
    n_samples = 500
    n_features = 50
    
    X = pd.DataFrame(
        np.random.randn(n_samples, n_features),
        columns=[f'Feature_{i}' for i in range(n_features)]
    )
    
    # åˆ›å»ºæ ‡ç­¾ï¼ˆAD vs CNï¼‰
    y = pd.Series(
        np.random.choice([0, 1], n_samples, p=[0.6, 0.4])
    )
    
    # åˆ›å»ºé¢„æµ‹æ¨¡å‹
    predictor = AIPredictionModel(model_type='ensemble')
    
    # å‡†å¤‡æ•°æ®
    X_train, X_test, y_train, y_test, le = predictor.prepare_data(X, y)
    
    # ç‰¹å¾é€‰æ‹©
    selected_features = predictor.feature_selection(X_train, y_train, method='rfe', k=20)
    X_train_selected = X_train[:, selected_features]
    X_test_selected = X_test[:, selected_features]
    feature_names_selected = [f'Feature_{i}' for i in range(n_features) if selected_features[i]]
    
    # åˆ’åˆ†éªŒè¯é›†
    X_train_final, X_val, y_train_final, y_val = train_test_split(
        X_train_selected, y_train, test_size=0.2, 
        random_state=42, stratify=y_train
    )
    
    # è®­ç»ƒç¥ç»ç½‘ç»œæ¨¡å‹
    predictor.train_deep_learning_model(
        X_train_final, y_train_final, X_val, y_val
    )
    
    # è®­ç»ƒä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹
    predictor.train_traditional_models(X_train_final, y_train_final)
    
    # æ„å»ºé›†æˆæ¨¡å‹
    predictor.build_ensemble_model(X_train_final, y_train_final)
    
    # è¯„ä¼°æ¨¡å‹
    performance = predictor.evaluate_models(X_test_selected, y_test)
    
    # è®¡ç®—SHAPå€¼
    predictor.compute_shap_values(X_test_selected, feature_names_selected)
    
    # å¯è§†åŒ–ç»“æœ
    predictor.visualize_results(X_test_selected, y_test, feature_names_selected,
                            output_dir='./results/AI_prediction_models')
    
    print("\n" + "=" * 60)
    print("æ¨¡å‹æ€§èƒ½æ€»ç»“:")
    print("=" * 60)
    print(performance)
    
    print("\n" + "=" * 60)
    print("âœ… AIé¢„æµ‹æ¨¡å‹æ¼”ç¤ºå®Œæˆï¼")
    print("=" * 60)


if __name__ == '__main__':
    demo_ai_prediction_models()