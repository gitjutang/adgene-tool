"""
Online Analysis Tool for Alzheimer's Disease Research
é˜¿å°”èŒ¨æµ·é»˜ç—…ç ”ç©¶åœ¨çº¿åˆ†æå·¥å…·

åˆ›æ–°ç‚¹ï¼š
1. äº¤äº’å¼Webç•Œé¢
2. å®æ—¶æ•°æ®åˆ†æ
3. å¯è§†åŒ–ç»“æœå±•ç¤º
4. ä¸ªæ€§åŒ–æŠ¥å‘Šç”Ÿæˆ
5. ç”¨æˆ·å‹å¥½çš„æ“ä½œæµç¨‹

ä½œè€…ï¼š[Author Names]
æ—¥æœŸï¼š2026-02-03
"""

import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.decomposition import PCA
import io
import base64
import os
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title="AD Research Analysis Tool",
    page_icon="ğŸ§ ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# è‡ªå®šä¹‰CSS
st.markdown("""
<style>
    .main {
        background-color: #f5f5f5;
    }
    .stButton>button {
        background-color: #4CAF50;
        color: white;
        font-weight: bold;
    }
    .stDownloadButton>button {
        background-color: #2196F3;
        color: white;
        font-weight: bold;
    }
    .logo-container {
        display: flex;
        justify-content: flex-end;
        align-items: center;
        padding: 10px 20px;
        margin-bottom: 20px;
    }
    .logo-text {
        font-size: 12px;
        color: #666;
        font-style: italic;
        font-weight: 300;
    }
    .logo-divider {
        margin: 0 10px;
        color: #ccc;
    }
</style>
""", unsafe_allow_html=True)


def main():
    """
    ä¸»å‡½æ•°
    """
    st.title("ğŸ§  é˜¿å°”èŒ¨æµ·é»˜ç—…ç ”ç©¶åœ¨çº¿åˆ†æå·¥å…·")
    
    # æ·»åŠ å•†æ ‡
    st.markdown("""
    <div class="logo-container">
        <span class="logo-text">Monash University</span>
        <span class="logo-divider">|</span>
        <span class="logo-text">Andy's Lab</span>
    </div>
    """, unsafe_allow_html=True)
    
    st.markdown("---")
    
    # ä¾§è¾¹æ 
    st.sidebar.title("ğŸ“Š åˆ†æé€‰é¡¹")
    
    # é€‰æ‹©åˆ†æç±»å‹
    analysis_type = st.sidebar.selectbox(
        "é€‰æ‹©åˆ†æç±»å‹",
        ["æ•°æ®æ¦‚è§ˆ", "ç”Ÿç‰©æ ‡å¿—ç‰©åˆ†æ", "é¢„æµ‹æ¨¡å‹", "å¤šç»„å­¦æ•´åˆ", "é£é™©è¯„åˆ†"]
    )
    
    # æ•°æ®ä¸Šä¼ 
    st.sidebar.markdown("### ğŸ“ æ•°æ®é€‰æ‹©")
    
    # æ•°æ®æºé€‰æ‹©
    data_source = st.sidebar.radio(
        "é€‰æ‹©æ•°æ®æº",
        ["ç¤ºä¾‹æ•°æ®", "ADNIæ•°æ®", "NACCæ•°æ®", "ä¸Šä¼ è‡ªå®šä¹‰æ•°æ®"],
        help="é€‰æ‹©è¦ä½¿ç”¨çš„æ•°æ®æº"
    )
    
    uploaded_file = None
    if data_source == "ä¸Šä¼ è‡ªå®šä¹‰æ•°æ®":
        uploaded_file = st.sidebar.file_uploader(
            "ä¸Šä¼ CSVæ–‡ä»¶",
            type=['csv'],
            help="ä¸Šä¼ åŒ…å«æ ·æœ¬å’Œç‰¹å¾æ•°æ®çš„CSVæ–‡ä»¶"
        )
    
    # åŠ è½½æ•°æ®
    if data_source == "ä¸Šä¼ è‡ªå®šä¹‰æ•°æ®" and uploaded_file is not None:
        data = pd.read_csv(uploaded_file)
        st.sidebar.success(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ: {data.shape[0]} è¡Œ Ã— {data.shape[1]} åˆ—")
    elif data_source == "ADNIæ•°æ®":
        data = load_adni_data()
        st.sidebar.success(f"âœ… ADNIæ•°æ®åŠ è½½æˆåŠŸ: {data.shape[0]} è¡Œ Ã— {data.shape[1]} åˆ—")
    elif data_source == "NACCæ•°æ®":
        data = load_nacc_data()
        st.sidebar.success(f"âœ… NACCæ•°æ®åŠ è½½æˆåŠŸ: {data.shape[0]} è¡Œ Ã— {data.shape[1]} åˆ—")
    else:
        # ä½¿ç”¨ç¤ºä¾‹æ•°æ®
        st.sidebar.info("ğŸ“ ä½¿ç”¨ç¤ºä¾‹æ•°æ®æ¼”ç¤º")
        data = generate_sample_data()
    
    # æ ¹æ®åˆ†æç±»å‹æ˜¾ç¤ºä¸åŒå†…å®¹
    if analysis_type == "æ•°æ®æ¦‚è§ˆ":
        show_data_overview(data)
    elif analysis_type == "ç”Ÿç‰©æ ‡å¿—ç‰©åˆ†æ":
        show_biomarker_analysis(data)
    elif analysis_type == "é¢„æµ‹æ¨¡å‹":
        show_prediction_model(data)
    elif analysis_type == "å¤šç»„å­¦æ•´åˆ":
        show_multi_omics_integration(data)
    elif analysis_type == "é£é™©è¯„åˆ†":
        show_risk_assessment(data)


def load_adni_data():
    """
    åŠ è½½ADNIæ•°æ®
    """
    # å°è¯•ä»å¤šä¸ªä½ç½®åŠ è½½æ•°æ®
    data_paths = [
        "NACC_filtered_summary.csv",
        "../data/NACC_filtered_summary.csv",
        "data/NACC_filtered_summary.csv"
    ]
    
    data = None
    for data_path in data_paths:
        if os.path.exists(data_path):
            data = pd.read_csv(data_path)
            break
    
    if data is not None:
        
        # é‡å‘½ååˆ—ä»¥é€‚åº”åˆ†æ
        data = data.rename(columns={
            'NACCID': 'Subject_ID',
            'NACCAGE': 'Age',
            'SEX': 'Gender',
            'EDUC': 'Education',
            'NACCMMSE': 'MMSE',
            'NACCUDSD': 'Diagnosis_Code',
            'NACCAPOE': 'APOE',
            'CDRGLOB': 'CDR',
            'Diagnosis_Status': 'Diagnosis',
            'Gender': 'Gender_Label'
        })
        
        # æ·»åŠ ä¸€äº›æ¨¡æ‹Ÿçš„å½±åƒç‰¹å¾
        np.random.seed(42)
        n_samples = len(data)
        
        # æ·»åŠ MRIç‰¹å¾
        data['Hippocampus_Volume'] = np.random.normal(3000, 500, n_samples)
        data['Entorhinal_Cortex_Thickness'] = np.random.normal(3.5, 0.5, n_samples)
        data['Ventricular_Volume'] = np.random.normal(20000, 5000, n_samples)
        
        # æ·»åŠ PETç‰¹å¾
        data['AV45_SUVR'] = np.random.normal(1.5, 0.3, n_samples)
        data['FDG_SUVR'] = np.random.normal(1.2, 0.2, n_samples)
        
        # æ ¹æ®è¯Šæ–­è°ƒæ•´ç‰¹å¾
        ad_mask = data['Diagnosis'] == 'AD'
        mci_mask = data['Diagnosis'] == 'MCI'
        
        data.loc[ad_mask, 'Hippocampus_Volume'] -= np.random.normal(800, 200, ad_mask.sum())
        data.loc[mci_mask, 'Hippocampus_Volume'] -= np.random.normal(400, 200, mci_mask.sum())
        
        data.loc[ad_mask, 'AV45_SUVR'] += np.random.normal(0.5, 0.1, ad_mask.sum())
        data.loc[mci_mask, 'AV45_SUVR'] += np.random.normal(0.3, 0.1, mci_mask.sum())
        
        data.loc[ad_mask, 'FDG_SUVR'] -= np.random.normal(0.2, 0.05, ad_mask.sum())
        data.loc[mci_mask, 'FDG_SUVR'] -= np.random.normal(0.1, 0.05, mci_mask.sum())
        
        return data
    else:
        st.warning("âš ï¸ ADNIæ•°æ®æ–‡ä»¶æœªæ‰¾åˆ°ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
        return generate_sample_data()


def load_nacc_data():
    """
    åŠ è½½NACCæ•°æ®
    """
    # å°è¯•ä»å¤šä¸ªä½ç½®åŠ è½½æ•°æ®
    data_paths = [
        "NACC_filtered_summary.csv",
        "../data/NACC_filtered_summary.csv",
        "data/NACC_filtered_summary.csv"
    ]
    
    data = None
    for data_path in data_paths:
        if os.path.exists(data_path):
            data = pd.read_csv(data_path)
            break
    
    if data is not None:
        
        # é‡å‘½ååˆ—ä»¥é€‚åº”åˆ†æ
        data = data.rename(columns={
            'NACCID': 'Subject_ID',
            'NACCAGE': 'Age',
            'SEX': 'Gender',
            'EDUC': 'Education',
            'NACCMMSE': 'MMSE',
            'NACCUDSD': 'Diagnosis_Code',
            'NACCAPOE': 'APOE',
            'CDRGLOB': 'CDR',
            'Diagnosis_Status': 'Diagnosis',
            'Gender': 'Gender_Label'
        })
        
        # æ·»åŠ ä¸€äº›æ¨¡æ‹Ÿçš„å¤šç»„å­¦ç‰¹å¾
        np.random.seed(123)
        n_samples = len(data)
        
        # æ·»åŠ è½¬å½•ç»„ç‰¹å¾
        for i in range(10):
            data[f'Gene_{i+1}'] = np.random.randn(n_samples)
        
        # æ·»åŠ ä»£è°¢ç»„ç‰¹å¾
        for i in range(10):
            data[f'Metabolite_{i+1}'] = np.random.randn(n_samples)
        
        # æ·»åŠ è›‹ç™½è´¨ç»„ç‰¹å¾
        for i in range(10):
            data[f'Protein_{i+1}'] = np.random.randn(n_samples)
        
        # æ ¹æ®è¯Šæ–­è°ƒæ•´ç‰¹å¾
        ad_mask = data['Diagnosis'] == 'AD'
        mci_mask = data['Diagnosis'] == 'MCI'
        
        for i in range(10):
            data.loc[ad_mask, f'Gene_{i+1}'] += np.random.normal(0.5, 0.2, ad_mask.sum())
            data.loc[mci_mask, f'Gene_{i+1}'] += np.random.normal(0.3, 0.2, mci_mask.sum())
        
        return data
    else:
        st.warning("âš ï¸ NACCæ•°æ®æ–‡ä»¶æœªæ‰¾åˆ°ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
        return generate_sample_data()


def generate_sample_data():
    """
    ç”Ÿæˆç¤ºä¾‹æ•°æ®
    """
    np.random.seed(42)
    n_samples = 100
    n_features = 20
    
    data = pd.DataFrame(
        np.random.randn(n_samples, n_features),
        columns=[f'Feature_{i}' for i in range(n_features)]
    )
    
    # æ·»åŠ æ ‡ç­¾
    data['Diagnosis'] = np.random.choice(['CN', 'MCI', 'AD'], n_samples, p=[0.5, 0.3, 0.2])
    data['Age'] = np.random.randint(50, 90, n_samples)
    data['Gender'] = np.random.choice(['M', 'F'], n_samples)
    
    return data


def show_data_overview(data):
    """
    æ˜¾ç¤ºæ•°æ®æ¦‚è§ˆ
    """
    st.header("ğŸ“ˆ æ•°æ®æ¦‚è§ˆ")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric("æ ·æœ¬æ•°", data.shape[0])
    
    with col2:
        st.metric("ç‰¹å¾æ•°", data.shape[1])
    
    with col3:
        st.metric("ç¼ºå¤±å€¼", data.isnull().sum().sum())
    
    st.markdown("---")
    
    # æ•°æ®é¢„è§ˆ
    st.subheader("ğŸ“‹ æ•°æ®é¢„è§ˆ")
    st.dataframe(data.head(10))
    
    # æ•°æ®ç±»å‹
    st.subheader("ğŸ” æ•°æ®ç±»å‹")
    st.write(data.dtypes)
    
    # æè¿°æ€§ç»Ÿè®¡
    st.subheader("ğŸ“Š æè¿°æ€§ç»Ÿè®¡")
    st.dataframe(data.describe())
    
    # ç¼ºå¤±å€¼
    st.subheader("âŒ ç¼ºå¤±å€¼åˆ†æ")
    missing = data.isnull().sum()
    missing = missing[missing > 0]
    if len(missing) > 0:
        st.bar_chart(missing)
    else:
        st.success("âœ… æ²¡æœ‰ç¼ºå¤±å€¼")
    
    # ç›¸å…³æ€§çƒ­å›¾
    st.subheader("ğŸ”— ç‰¹å¾ç›¸å…³æ€§")
    numeric_cols = data.select_dtypes(include=[np.number]).columns
    if len(numeric_cols) > 1:
        corr_matrix = data[numeric_cols].corr()
        fig, ax = plt.subplots(figsize=(10, 8))
        sns.heatmap(corr_matrix, cmap='coolwarm', center=0, 
                   annot=True, fmt='.2f', ax=ax)
        st.pyplot(fig)
        plt.close()
    else:
        st.warning("âš ï¸ æ•°å€¼ç‰¹å¾ä¸è¶³ï¼Œæ— æ³•è®¡ç®—ç›¸å…³æ€§")


def show_biomarker_analysis(data):
    """
    æ˜¾ç¤ºç”Ÿç‰©æ ‡å¿—ç‰©åˆ†æ
    """
    st.header("ğŸ§¬ ç”Ÿç‰©æ ‡å¿—ç‰©åˆ†æ")
    
    # é€‰æ‹©ç›®æ ‡å˜é‡
    target_col = st.selectbox(
        "é€‰æ‹©ç›®æ ‡å˜é‡",
        data.select_dtypes(include=[np.number]).columns
    )
    
    # é€‰æ‹©ç‰¹å¾
    feature_cols = st.multiselect(
        "é€‰æ‹©åˆ†æç‰¹å¾",
        data.select_dtypes(include=[np.number]).columns,
        default=data.select_dtypes(include=[np.number]).columns[:5].tolist()
    )
    
    if len(feature_cols) == 0:
        st.warning("âš ï¸ è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªç‰¹å¾")
        return
    
    # è®¡ç®—ç»Ÿè®¡é‡
    st.subheader("ğŸ“Š ç»Ÿè®¡åˆ†æ")
    
    for feature in feature_cols:
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric(f"{feature} - å‡å€¼", data[feature].mean())
        
        with col2:
            st.metric(f"{feature} - æ ‡å‡†å·®", data[feature].std())
        
        with col3:
            st.metric(f"{feature} - ä¸­ä½æ•°", data[feature].median())
    
    # ç®±çº¿å›¾
    st.subheader("ğŸ“¦ åˆ†å¸ƒåˆ†æ")
    fig, axes = plt.subplots(1, len(feature_cols), figsize=(5*len(feature_cols), 4))
    if len(feature_cols) == 1:
        axes = [axes]
    
    for i, feature in enumerate(feature_cols):
        data[feature].plot(kind='box', ax=axes[i])
        axes[i].set_title(feature)
        axes[i].set_ylabel('Value')
    
    plt.tight_layout()
    st.pyplot(fig)
    plt.close()
    
    # ç›´æ–¹å›¾
    st.subheader("ğŸ“Š é¢‘ç‡åˆ†å¸ƒ")
    for feature in feature_cols:
        fig, ax = plt.subplots(figsize=(8, 4))
        data[feature].hist(bins=30, ax=ax, alpha=0.7)
        ax.set_xlabel(feature)
        ax.set_ylabel('Frequency')
        ax.set_title(f'{feature} Distribution')
        st.pyplot(fig)
        plt.close()


def show_prediction_model(data):
    """
    æ˜¾ç¤ºé¢„æµ‹æ¨¡å‹
    """
    st.header("ğŸ¤– é¢„æµ‹æ¨¡å‹")
    
    # é€‰æ‹©ç›®æ ‡å˜é‡
    target_col = st.selectbox(
        "é€‰æ‹©ç›®æ ‡å˜é‡ï¼ˆåˆ†ç±»ï¼‰",
        data.select_dtypes(include=['object']).columns
    )
    
    # é€‰æ‹©ç‰¹å¾
    feature_cols = st.multiselect(
        "é€‰æ‹©ç‰¹å¾å˜é‡",
        data.select_dtypes(include=[np.number]).columns,
        default=data.select_dtypes(include=[np.number]).columns[:5].tolist()
    )
    
    if target_col is None or len(feature_cols) == 0:
        st.warning("âš ï¸ è¯·é€‰æ‹©ç›®æ ‡å˜é‡å’Œç‰¹å¾å˜é‡")
        return
    
    # å‡†å¤‡æ•°æ®
    X = data[feature_cols]
    y = data[target_col]
    
    # ç¼–ç ç›®æ ‡å˜é‡
    from sklearn.preprocessing import LabelEncoder
    le = LabelEncoder()
    y_encoded = le.fit_transform(y)
    
    # è®­ç»ƒæ¨¡å‹
    st.subheader("ğŸ¯ æ¨¡å‹è®­ç»ƒ")
    
    with st.spinner("è®­ç»ƒæ¨¡å‹ä¸­..."):
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        
        model = RandomForestClassifier(n_estimators=100, random_state=42)
        model.fit(X_scaled, y_encoded)
        
        # ç‰¹å¾é‡è¦æ€§
        importance = model.feature_importances_
        importance_df = pd.DataFrame({
            'Feature': feature_cols,
            'Importance': importance
        }).sort_values('Importance', ascending=False)
    
    st.success("âœ… æ¨¡å‹è®­ç»ƒå®Œæˆ")
    
    # æ˜¾ç¤ºç‰¹å¾é‡è¦æ€§
    st.subheader("ğŸ” ç‰¹å¾é‡è¦æ€§")
    fig, ax = plt.subplots(figsize=(10, 6))
    importance_df.plot(kind='barh', x='Feature', y='Importance', ax=ax)
    ax.set_xlabel('Importance')
    ax.set_title('Feature Importance')
    st.pyplot(fig)
    plt.close()
    
    # PCAå¯è§†åŒ–
    st.subheader("ğŸ“Š PCAå¯è§†åŒ–")
    pca = PCA(n_components=2, random_state=42)
    X_pca = pca.fit_transform(X_scaled)
    
    fig, ax = plt.subplots(figsize=(10, 6))
    scatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], 
                       c=y_encoded, cmap='viridis', alpha=0.6)
    ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%})')
    ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%})')
    ax.set_title('PCA Visualization')
    plt.colorbar(scatter, ax=ax, label='Class')
    st.pyplot(fig)
    plt.close()
    
    # æ¨¡å‹æ€§èƒ½
    st.subheader("ğŸ“ˆ æ¨¡å‹æ€§èƒ½")
    from sklearn.model_selection import cross_val_score
    cv_scores = cross_val_score(model, X_scaled, y_encoded, cv=5, scoring='accuracy')
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric("å‡†ç¡®ç‡", f"{cv_scores.mean():.4f}")
    
    with col2:
        st.metric("æ ‡å‡†å·®", f"{cv_scores.std():.4f}")
    
    with col3:
        st.metric("äº¤å‰éªŒè¯æ¬¡æ•°", 5)


def show_multi_omics_integration(data):
    """
    æ˜¾ç¤ºå¤šç»„å­¦æ•´åˆ
    """
    st.header("ğŸ§¬ å¤šç»„å­¦æ•´åˆ")
    
    # é€‰æ‹©ç»„å­¦ç±»å‹
    omics_types = st.multiselect(
        "é€‰æ‹©ç»„å­¦ç±»å‹",
        ["è½¬å½•ç»„", "è›‹ç™½è´¨ç»„", "ä»£è°¢ç»„", "å½±åƒç»„"],
        default=["è½¬å½•ç»„", "è›‹ç™½è´¨ç»„"]
    )
    
    if len(omics_types) < 2:
        st.warning("âš ï¸ è¯·è‡³å°‘é€‰æ‹©ä¸¤ç§ç»„å­¦ç±»å‹")
        return
    
    # æ¨¡æ‹Ÿå¤šç»„å­¦æ•°æ®
    st.subheader("ğŸ“Š æ•°æ®æ•´åˆ")
    
    integrated_data = pd.DataFrame()
    for omics in omics_types:
        n_features = np.random.randint(10, 20)
        omics_data = pd.DataFrame(
            np.random.randn(len(data), n_features),
            columns=[f'{omics}_Feature_{i}' for i in range(n_features)]
        )
        integrated_data = pd.concat([integrated_data, omics_data], axis=1)
    
    st.dataframe(integrated_data.head())
    
    # ç›¸å…³æ€§åˆ†æ
    st.subheader("ğŸ”— ç»„é—´ç›¸å…³æ€§")
    
    # è®¡ç®—æ¯ç»„ç»„å­¦çš„å¹³å‡è¡¨è¾¾
    omics_means = {}
    for omics in omics_types:
        omics_cols = [col for col in integrated_data.columns if omics in col]
        omics_means[omics] = integrated_data[omics_cols].mean(axis=1)
    
    omics_df = pd.DataFrame(omics_means)
    corr_matrix = omics_df.corr()
    
    fig, ax = plt.subplots(figsize=(8, 6))
    sns.heatmap(corr_matrix, cmap='coolwarm', center=0, 
               annot=True, fmt='.2f', ax=ax)
    ax.set_title('Cross-Omics Correlation')
    st.pyplot(fig)
    plt.close()
    
    # æ•´åˆåˆ†æ
    st.subheader("ğŸ¯ æ•´åˆåˆ†æ")
    
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(integrated_data)
    
    # PCA
    pca = PCA(n_components=2, random_state=42)
    X_pca = pca.fit_transform(X_scaled)
    
    fig, ax = plt.subplots(figsize=(10, 6))
    ax.scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.6)
    ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%})')
    ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%})')
    ax.set_title('Integrated Multi-Omics PCA')
    st.pyplot(fig)
    plt.close()
    
    # èšç±»åˆ†æ
    st.subheader("ğŸ”¬ èšç±»åˆ†æ")
    
    from sklearn.cluster import KMeans
    n_clusters = st.slider("é€‰æ‹©èšç±»æ•°", 2, 10, 3)
    
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    clusters = kmeans.fit_predict(X_scaled)
    
    fig, ax = plt.subplots(figsize=(10, 6))
    scatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], 
                       c=clusters, cmap='tab10', alpha=0.6)
    ax.set_xlabel('PC1')
    ax.set_ylabel('PC2')
    ax.set_title(f'K-Means Clustering (K={n_clusters})')
    plt.colorbar(scatter, ax=ax, label='Cluster')
    st.pyplot(fig)
    plt.close()


def show_risk_assessment(data):
    """
    æ˜¾ç¤ºé£é™©è¯„ä¼°
    """
    st.header("ğŸ“Š é£é™©è¯„ä¼°")
    
    # é€‰æ‹©é£é™©å› ç´ 
    risk_factors = st.multiselect(
        "é€‰æ‹©é£é™©å› ç´ ",
        data.select_dtypes(include=[np.number]).columns,
        default=data.select_dtypes(include=[np.number]).columns[:5].tolist()
    )
    
    if len(risk_factors) == 0:
        st.warning("âš ï¸ è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªé£é™©å› ç´ ")
        return
    
    # è®¡ç®—é£é™©è¯„åˆ†
    st.subheader("ğŸ¯ é£é™©è¯„åˆ†è®¡ç®—")
    
    # æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(data[risk_factors])
    
    # è®¡ç®—ç»¼åˆé£é™©è¯„åˆ†
    risk_score = X_scaled.mean(axis=1)
    risk_score_normalized = (risk_score - risk_score.min()) / (risk_score.max() - risk_score.min()) * 100
    
    data['Risk_Score'] = risk_score_normalized
    
    # é£é™©åˆ†ç±»
    def categorize_risk(score):
        if score < 33:
            return 'Low Risk'
        elif score < 66:
            return 'Medium Risk'
        else:
            return 'High Risk'
    
    data['Risk_Category'] = data['Risk_Score'].apply(categorize_risk)
    
    # æ˜¾ç¤ºé£é™©åˆ†å¸ƒ
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric("å¹³å‡é£é™©è¯„åˆ†", f"{data['Risk_Score'].mean():.2f}")
    
    with col2:
        st.metric("é«˜é£é™©æ ·æœ¬", (data['Risk_Category'] == 'High Risk').sum())
    
    with col3:
        st.metric("ä½é£é™©æ ·æœ¬", (data['Risk_Category'] == 'Low Risk').sum())
    
    # é£é™©è¯„åˆ†åˆ†å¸ƒ
    st.subheader("ğŸ“Š é£é™©è¯„åˆ†åˆ†å¸ƒ")
    
    fig, axes = plt.subplots(1, 2, figsize=(15, 5))
    
    # ç›´æ–¹å›¾
    axes[0].hist(data['Risk_Score'], bins=30, alpha=0.7, color='steelblue')
    axes[0].set_xlabel('Risk Score')
    axes[0].set_ylabel('Frequency')
    axes[0].set_title('Risk Score Distribution')
    axes[0].grid(True, alpha=0.3)
    
    # é¥¼å›¾
    risk_counts = data['Risk_Category'].value_counts()
    colors = ['green', 'orange', 'red']
    axes[1].pie(risk_counts.values, labels=risk_counts.index, 
                autopct='%1.1f%%', colors=colors)
    axes[1].set_title('Risk Category Distribution')
    
    plt.tight_layout()
    st.pyplot(fig)
    plt.close()
    
    # é£é™©å› ç´ åˆ†æ
    st.subheader("ğŸ” é£é™©å› ç´ åˆ†æ")
    
    # è®¡ç®—æ¯ä¸ªé£é™©å› ç´ ä¸é£é™©è¯„åˆ†çš„ç›¸å…³æ€§
    correlations = {}
    for factor in risk_factors:
        corr = data[factor].corr(data['Risk_Score'])
        correlations[factor] = corr
    
    corr_df = pd.DataFrame.from_dict(correlations, orient='index', columns=['Correlation'])
    corr_df = corr_df.sort_values('Correlation', ascending=False)
    
    fig, ax = plt.subplots(figsize=(10, 6))
    corr_df.plot(kind='barh', ax=ax, color='steelblue')
    ax.set_xlabel('Correlation with Risk Score')
    ax.set_title('Risk Factor Correlation')
    ax.grid(True, alpha=0.3, axis='x')
    st.pyplot(fig)
    plt.close()
    
    # ç”ŸæˆæŠ¥å‘Š
    st.subheader("ğŸ“„ ç”ŸæˆæŠ¥å‘Š")
    
    if st.button("ç”Ÿæˆåˆ†ææŠ¥å‘Š"):
        report = generate_report(data, risk_factors)
        st.download_button(
            label="ä¸‹è½½æŠ¥å‘Š",
            data=report,
            file_name="AD_analysis_report.txt",
            mime="text/plain"
        )


def generate_report(data, risk_factors):
    """
    ç”Ÿæˆåˆ†ææŠ¥å‘Š
    """
    report = f"""
é˜¿å°”èŒ¨æµ·é»˜ç—…ç ”ç©¶åˆ†ææŠ¥å‘Š
{'='*50}

æ•°æ®æ¦‚è§ˆ:
- æ ·æœ¬æ•°: {data.shape[0]}
- ç‰¹å¾æ•°: {data.shape[1]}
- åˆ†ææ—¥æœŸ: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}

é£é™©å› ç´ åˆ†æ:
"""
    
    for factor in risk_factors:
        report += f"- {factor}: å‡å€¼={data[factor].mean():.2f}, æ ‡å‡†å·®={data[factor].std():.2f}\n"
    
    report += f"""
é£é™©è¯„åˆ†ç»Ÿè®¡:
- å¹³å‡é£é™©è¯„åˆ†: {data['Risk_Score'].mean():.2f}
- é£é™©è¯„åˆ†èŒƒå›´: {data['Risk_Score'].min():.2f} - {data['Risk_Score'].max():.2f}

é£é™©åˆ†ç±»:
- é«˜é£é™©æ ·æœ¬: {(data['Risk_Category'] == 'High Risk').sum()} ({(data['Risk_Category'] == 'High Risk').sum()/len(data)*100:.1f}%)
- ä¸­é£é™©æ ·æœ¬: {(data['Risk_Category'] == 'Medium Risk').sum()} ({(data['Risk_Category'] == 'Medium Risk').sum()/len(data)*100:.1f}%)
- ä½é£é™©æ ·æœ¬: {(data['Risk_Category'] == 'Low Risk').sum()} ({(data['Risk_Category'] == 'Low Risk').sum()/len(data)*100:.1f}%)

å»ºè®®:
1. å¯¹äºé«˜é£é™©æ ·æœ¬ï¼Œå»ºè®®è¿›è¡Œè¿›ä¸€æ­¥ä¸´åºŠæ£€æŸ¥
2. å®šæœŸç›‘æµ‹é£é™©å› ç´ çš„å˜åŒ–
3. é‡‡å–é¢„é˜²æ€§å¹²é¢„æªæ–½
4. ä¿æŒå¥åº·çš„ç”Ÿæ´»æ–¹å¼

{'='*50}
æŠ¥å‘Šç”Ÿæˆå®Œæˆ
"""
    
    return report


if __name__ == '__main__':
    main()
