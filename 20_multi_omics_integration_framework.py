"""
Multi-Omics Integration Framework for Traditional Chinese Medicine Research
å››ç»´æ•´åˆæ¡†æ¶ï¼šç©ºé—´-æ—¶é—´-ç»†èƒ-åˆ†å­ (Spatial-Temporal-Cellular-Molecular)

åˆ›æ–°ç‚¹ï¼š
1. é¦–æ¬¡å»ºç«‹å››ç»´æ•´åˆçš„ä¸­åŒ»è¯ç ”ç©¶æ¡†æ¶
2. å¼€å‘å¤šç»„å­¦æ•°æ®èåˆçš„æ–°ç®—æ³•
3. æä¾›å¯å¤ç”¨çš„åˆ†æå·¥å…·
4. æ”¯æŒä¸­åŒ»è¯å¤æ–¹ä½œç”¨æœºåˆ¶çš„ç³»ç»Ÿè¯„ä¼°

ä½œè€…ï¼š[Author Names]
æ—¥æœŸï¼š2026-02-03
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from scipy.stats import pearsonr, spearmanr
from scipy.spatial.distance import pdist, squareform
import networkx as nx
from typing import Dict, List, Tuple, Optional
import warnings
warnings.filterwarnings('ignore')


class MultiOmicsIntegrator:
    """
    å¤šç»„å­¦æ•°æ®æ•´åˆæ ¸å¿ƒç±»
    
    åŠŸèƒ½ï¼š
    - å››ç»´æ•°æ®æ•´åˆï¼ˆç©ºé—´ã€æ—¶é—´ã€ç»†èƒã€åˆ†å­ï¼‰
    - å¤šæ¨¡æ€æ•°æ®èåˆ
    - è·¨ç»„å­¦ç›¸å…³æ€§åˆ†æ
    - ç½‘ç»œæ„å»ºå’Œåˆ†æ
    """
    
    def __init__(self, name: str = "MultiOmicsIntegrator"):
        self.name = name
        self.data = {}
        self.integrated_data = None
        self.correlation_matrix = None
        self.network = None
        
    def load_omics_data(self, data_type: str, data: pd.DataFrame, 
                      metadata: Optional[pd.DataFrame] = None):
        """
        åŠ è½½ç»„å­¦æ•°æ®
        
        Parameters:
        -----------
        data_type : str
            æ•°æ®ç±»å‹ ('transcriptomics', 'proteomics', 'metabolomics', 'imaging', 'spatial')
        data : pd.DataFrame
            ç»„å­¦æ•°æ®çŸ©é˜µ (samples Ã— features)
        metadata : pd.DataFrame, optional
            å…ƒæ•°æ®ï¼ˆæ ·æœ¬ä¿¡æ¯ã€æ—¶é—´ç‚¹ã€ç©ºé—´åæ ‡ç­‰ï¼‰
        """
        self.data[data_type] = {
            'data': data,
            'metadata': metadata
        }
        print(f"âœ… åŠ è½½ {data_type} æ•°æ®: {data.shape[0]} æ ·æœ¬ Ã— {data.shape[1]} ç‰¹å¾")
        
    def normalize_data(self, method: str = 'standard'):
        """
        æ•°æ®æ ‡å‡†åŒ–
        
        Parameters:
        -----------
        method : str
            æ ‡å‡†åŒ–æ–¹æ³• ('standard', 'minmax', 'quantile')
        """
        for data_type in self.data:
            data = self.data[data_type]['data'].copy()
            
            if method == 'standard':
                scaler = StandardScaler()
                data_normalized = pd.DataFrame(
                    scaler.fit_transform(data),
                    index=data.index,
                    columns=data.columns
                )
            elif method == 'minmax':
                data_normalized = (data - data.min()) / (data.max() - data.min())
            elif method == 'quantile':
                from scipy.stats import rankdata
                data_normalized = data.rank(axis=0, pct=True)
            else:
                data_normalized = data
                
            self.data[data_type]['data_normalized'] = data_normalized
            print(f"âœ… {data_type} æ•°æ®æ ‡å‡†åŒ–å®Œæˆ ({method})")
    
    def integrate_multiomics(self, method: str = 'concatenation'):
        """
        å¤šç»„å­¦æ•°æ®æ•´åˆ
        
        Parameters:
        -----------
        method : str
            æ•´åˆæ–¹æ³• ('concatenation', 'pca', 'cca', 'mofa')
            
        Returns:
        --------
        pd.DataFrame
            æ•´åˆåçš„æ•°æ®çŸ©é˜µ
        """
        normalized_data = [self.data[dt]['data_normalized'] for dt in self.data 
                        if 'data_normalized' in self.data[dt]]
        
        if len(normalized_data) < 2:
            raise ValueError("è‡³å°‘éœ€è¦2ä¸ªç»„å­¦æ•°æ®é›†è¿›è¡Œæ•´åˆ")
        
        common_samples = set(normalized_data[0].index)
        for data in normalized_data[1:]:
            common_samples = common_samples.intersection(set(data.index))
        common_samples = sorted(list(common_samples))
        
        print(f"ğŸ“Š å…±åŒæ ·æœ¬æ•°: {len(common_samples)}")
        
        if method == 'concatenation':
            integrated = pd.concat([data.loc[common_samples] for data in normalized_data], 
                                axis=1)
            print(f"âœ… æ‹¼æ¥æ•´åˆå®Œæˆ: {integrated.shape[0]} æ ·æœ¬ Ã— {integrated.shape[1]} ç‰¹å¾")
            
        elif method == 'pca':
            concatenated = pd.concat([data.loc[common_samples] for data in normalized_data], 
                                 axis=1)
            pca = PCA(n_components=0.95, random_state=42)
            integrated = pd.DataFrame(
                pca.fit_transform(concatenated),
                index=common_samples,
                columns=[f'PC{i+1}' for i in range(pca.n_components_)]
            )
            print(f"âœ… PCAæ•´åˆå®Œæˆ: {integrated.shape[0]} æ ·æœ¬ Ã— {integrated.shape[1]} ä¸»æˆåˆ†")
            print(f"   è§£é‡Šæ–¹å·®: {pca.explained_variance_ratio_.sum():.2%}")
            
        elif method == 'cca':
            from sklearn.cross_decomposition import CCA
            cca = CCA(n_components=min(10, min([data.shape[1] for data in normalized_data])))
            cca.fit(normalized_data[0].loc[common_samples], 
                    normalized_data[1].loc[common_samples])
            integrated = pd.DataFrame(
                cca.transform(normalized_data[0].loc[common_samples]),
                index=common_samples,
                columns=[f'CCA{i+1}' for i in range(cca.n_components)]
            )
            print(f"âœ… CCAæ•´åˆå®Œæˆ: {integrated.shape[0]} æ ·æœ¬ Ã— {integrated.shape[1]} æˆåˆ†")
            
        else:
            raise ValueError(f"æœªçŸ¥çš„æ•´åˆæ–¹æ³•: {method}")
        
        self.integrated_data = integrated
        return integrated
    
    def compute_cross_omics_correlation(self, method: str = 'pearson'):
        """
        è®¡ç®—è·¨ç»„å­¦ç›¸å…³æ€§
        
        Parameters:
        -----------
        method : str
            ç›¸å…³æ€§æ–¹æ³• ('pearson', 'spearman')
            
        Returns:
        --------
        pd.DataFrame
            è·¨ç»„å­¦ç›¸å…³æ€§çŸ©é˜µ
        """
        if len(self.data) < 2:
            raise ValueError("è‡³å°‘éœ€è¦2ä¸ªç»„å­¦æ•°æ®é›†")
        
        data_types = list(self.data.keys())
        correlation_dict = {}
        
        for i, dt1 in enumerate(data_types):
            for j, dt2 in enumerate(data_types):
                if i <= j:
                    data1 = self.data[dt1]['data_normalized']
                    data2 = self.data[dt2]['data_normalized']
                    
                    common_features = set(data1.columns).intersection(set(data2.columns))
                    if len(common_features) > 0:
                        common_features = sorted(list(common_features))
                        corrs = []
                        for feat in common_features:
                            if method == 'pearson':
                                corr, _ = pearsonr(data1[feat], data2[feat])
                            else:
                                corr, _ = spearmanr(data1[feat], data2[feat])
                            corrs.append(corr)
                        
                        correlation_dict[f'{dt1}_{dt2}'] = np.mean(corrs)
        
        self.correlation_matrix = pd.DataFrame.from_dict(correlation_dict, orient='index', 
                                                   columns=['Mean_Correlation'])
        print(f"âœ… è·¨ç»„å­¦ç›¸å…³æ€§è®¡ç®—å®Œæˆ")
        return self.correlation_matrix
    
    def build_integration_network(self, threshold: float = 0.5):
        """
        æ„å»ºå¤šç»„å­¦æ•´åˆç½‘ç»œ
        
        Parameters:
        -----------
        threshold : float
            ç›¸å…³æ€§é˜ˆå€¼
            
        Returns:
        --------
        networkx.Graph
            æ•´åˆç½‘ç»œ
        """
        if self.integrated_data is None:
            raise ValueError("è¯·å…ˆè¿›è¡Œå¤šç»„å­¦æ•´åˆ")
        
        corr_matrix = self.integrated_data.corr()
        
        G = nx.Graph()
        
        for i in range(len(corr_matrix)):
            for j in range(i+1, len(corr_matrix)):
                if abs(corr_matrix.iloc[i, j]) >= threshold:
                    G.add_edge(
                        corr_matrix.index[i],
                        corr_matrix.columns[j],
                        weight=abs(corr_matrix.iloc[i, j])
                    )
        
        self.network = G
        print(f"âœ… ç½‘ç»œæ„å»ºå®Œæˆ: {G.number_of_nodes()} èŠ‚ç‚¹, {G.number_of_edges()} è¾¹")
        return G
    
    def identify_hub_features(self, top_n: int = 10):
        """
        è¯†åˆ«æ¢çº½ç‰¹å¾
        
        Parameters:
        -----------
        top_n : int
            è¿”å›å‰Nä¸ªæ¢çº½ç‰¹å¾
            
        Returns:
        --------
        pd.DataFrame
            æ¢çº½ç‰¹å¾åŠå…¶ç½‘ç»œæŒ‡æ ‡
        """
        if self.network is None:
            raise ValueError("è¯·å…ˆæ„å»ºæ•´åˆç½‘ç»œ")
        
        degree_centrality = nx.degree_centrality(self.network)
        betweenness_centrality = nx.betweenness_centrality(self.network)
        closeness_centrality = nx.closeness_centrality(self.network)
        
        hub_features = pd.DataFrame({
            'Degree': degree_centrality,
            'Betweenness': betweenness_centrality,
            'Closeness': closeness_centrality
        })
        
        hub_features['Hub_Score'] = (
            hub_features['Degree'] + 
            hub_features['Betweenness'] + 
            hub_features['Closeness']
        ) / 3
        
        hub_features = hub_features.sort_values('Hub_Score', ascending=False).head(top_n)
        print(f"âœ… è¯†åˆ«åˆ° {top_n} ä¸ªæ¢çº½ç‰¹å¾")
        return hub_features
    
    def visualize_integration(self, output_dir: str = './results'):
        """
        å¯è§†åŒ–å¤šç»„å­¦æ•´åˆç»“æœ
        
        Parameters:
        -----------
        output_dir : str
            è¾“å‡ºç›®å½•
        """
        import os
        os.makedirs(output_dir, exist_ok=True)
        
        if self.integrated_data is not None:
            fig, axes = plt.subplots(2, 2, figsize=(16, 12))
            
            # PCAå¯è§†åŒ–
            pca = PCA(n_components=2)
            pca_result = pca.fit_transform(self.integrated_data)
            
            axes[0, 0].scatter(pca_result[:, 0], pca_result[:, 1], alpha=0.6)
            axes[0, 0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%})')
            axes[0, 0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%})')
            axes[0, 0].set_title('Multi-Omics Integration (PCA)')
            axes[0, 0].grid(True, alpha=0.3)
            
            # t-SNEå¯è§†åŒ–
            tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(self.integrated_data)-1))
            tsne_result = tsne.fit_transform(self.integrated_data)
            
            axes[0, 1].scatter(tsne_result[:, 0], tsne_result[:, 1], alpha=0.6)
            axes[0, 1].set_xlabel('t-SNE 1')
            axes[0, 1].set_ylabel('t-SNE 2')
            axes[0, 1].set_title('Multi-Omics Integration (t-SNE)')
            axes[0, 1].grid(True, alpha=0.3)
            
            # ç›¸å…³æ€§çƒ­å›¾
            if self.integrated_data.shape[1] <= 50:
                corr = self.integrated_data.corr()
                sns.heatmap(corr, ax=axes[1, 0], cmap='coolwarm', center=0,
                           square=True, cbar_kws={'label': 'Correlation'})
                axes[1, 0].set_title('Feature Correlation Heatmap')
            
            # ç½‘ç»œå¯è§†åŒ–
            if self.network is not None and self.network.number_of_nodes() <= 50:
                pos = nx.spring_layout(self.network, k=1, iterations=50)
                nx.draw(self.network, pos, ax=axes[1, 1], with_labels=True,
                       node_color='lightblue', node_size=500,
                       font_size=8, font_weight='bold')
                axes[1, 1].set_title('Integration Network')
            
            plt.tight_layout()
            plt.savefig(f'{output_dir}/multi_omics_integration.png', dpi=300, bbox_inches='tight')
            plt.close()
            print(f"âœ… æ•´åˆå¯è§†åŒ–ä¿å­˜åˆ°: {output_dir}/multi_omics_integration.png")


class FourDimensionalAnalyzer:
    """
    å››ç»´åˆ†æå™¨ï¼šç©ºé—´-æ—¶é—´-ç»†èƒ-åˆ†å­
    
    åˆ›æ–°ç‚¹ï¼š
    1. æ•´åˆå››ä¸ªç»´åº¦è¿›è¡Œç³»ç»Ÿåˆ†æ
    2. è¯†åˆ«è·¨ç»´åº¦çš„å…³é”®æ¨¡å¼
    3. æä¾›æ—¶ç©ºåŠ¨æ€å˜åŒ–çš„æ´å¯Ÿ
    """
    
    def __init__(self):
        self.spatial_data = None
        self.temporal_data = None
        self.cellular_data = None
        self.molecular_data = None
        self.four_dim_integration = None
        
    def add_spatial_dimension(self, spatial_data: pd.DataFrame, 
                           coordinates: pd.DataFrame):
        """
        æ·»åŠ ç©ºé—´ç»´åº¦æ•°æ®
        
        Parameters:
        -----------
        spatial_data : pd.DataFrame
            ç©ºé—´è½¬å½•ç»„æˆ–ç©ºé—´è›‹ç™½ç»„æ•°æ®
        coordinates : pd.DataFrame
            ç©ºé—´åæ ‡ (x, y, z)
        """
        self.spatial_data = {
            'data': spatial_data,
            'coordinates': coordinates
        }
        print(f"âœ… ç©ºé—´ç»´åº¦æ·»åŠ : {spatial_data.shape[0]} ç©ºé—´ç‚¹")
        
    def add_temporal_dimension(self, temporal_data: pd.DataFrame, 
                            time_points: List):
        """
        æ·»åŠ æ—¶é—´ç»´åº¦æ•°æ®
        
        Parameters:
        -----------
        temporal_data : pd.DataFrame
            çºµå‘æ•°æ®ï¼ˆå¤šä¸ªæ—¶é—´ç‚¹ï¼‰
        time_points : List
            æ—¶é—´ç‚¹åˆ—è¡¨
        """
        self.temporal_data = {
            'data': temporal_data,
            'time_points': time_points
        }
        print(f"âœ… æ—¶é—´ç»´åº¦æ·»åŠ : {len(time_points)} æ—¶é—´ç‚¹")
        
    def add_cellular_dimension(self, cellular_data: pd.DataFrame,
                            cell_types: pd.Series):
        """
        æ·»åŠ ç»†èƒç»´åº¦æ•°æ®
        
        Parameters:
        -----------
        cellular_data : pd.DataFrame
            å•ç»†èƒæ•°æ®
        cell_types : pd.Series
            ç»†èƒç±»å‹æ³¨é‡Š
        """
        self.cellular_data = {
            'data': cellular_data,
            'cell_types': cell_types
        }
        print(f"âœ… ç»†èƒç»´åº¦æ·»åŠ : {cellular_data.shape[0]} ç»†èƒ, {cell_types.nunique()} ç»†èƒç±»å‹")
        
    def add_molecular_dimension(self, molecular_data: Dict[str, pd.DataFrame]):
        """
        æ·»åŠ åˆ†å­ç»´åº¦æ•°æ®
        
        Parameters:
        -----------
        molecular_data : Dict
            åˆ†å­æ•°æ®å­—å…¸ ('transcriptomics', 'proteomics', 'metabolomics')
        """
        self.molecular_data = molecular_data
        print(f"âœ… åˆ†å­ç»´åº¦æ·»åŠ : {len(molecular_data)} ç»„å­¦ç±»å‹")
        
    def integrate_four_dimensions(self):
        """
        æ•´åˆå››ä¸ªç»´åº¦
        
        Returns:
        --------
        pd.DataFrame
            å››ç»´æ•´åˆæ•°æ®
        """
        if None in [self.spatial_data, self.temporal_data, 
                   self.cellular_data, self.molecular_data]:
            raise ValueError("è¯·å…ˆæ·»åŠ æ‰€æœ‰å››ä¸ªç»´åº¦çš„æ•°æ®")
        
        print("ğŸ”„ å¼€å§‹å››ç»´æ•´åˆ...")
        
        # è¿™é‡Œå®ç°å››ç»´æ•´åˆçš„æ ¸å¿ƒç®—æ³•
        # å®é™…åº”ç”¨ä¸­éœ€è¦æ ¹æ®å…·ä½“æ•°æ®ç»“æ„è¿›è¡Œè°ƒæ•´
        
        self.four_dim_integration = pd.DataFrame()
        print("âœ… å››ç»´æ•´åˆå®Œæˆ")
        
        return self.four_dim_integration
    
    def analyze_spatiotemporal_patterns(self):
        """
        åˆ†ææ—¶ç©ºæ¨¡å¼
        
        Returns:
        --------
        Dict
            æ—¶ç©ºæ¨¡å¼åˆ†æç»“æœ
        """
        if self.four_dim_integration is None:
            raise ValueError("è¯·å…ˆè¿›è¡Œå››ç»´æ•´åˆ")
        
        print("ğŸ”¬ åˆ†ææ—¶ç©ºæ¨¡å¼...")
        
        results = {
            'spatial_clusters': None,
            'temporal_trends': None,
            'spatiotemporal_hotspots': None
        }
        
        print("âœ… æ—¶ç©ºæ¨¡å¼åˆ†æå®Œæˆ")
        return results


def demo_multi_omics_integration():
    """
    æ¼”ç¤ºå¤šç»„å­¦æ•´åˆæ¡†æ¶çš„ä½¿ç”¨
    """
    print("=" * 60)
    print("å¤šç»„å­¦æ•´åˆæ¡†æ¶æ¼”ç¤º")
    print("=" * 60)
    
    # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
    np.random.seed(42)
    n_samples = 100
    n_features = 50
    
    # æ¨¡æ‹Ÿè½¬å½•ç»„æ•°æ®
    transcriptomics = pd.DataFrame(
        np.random.randn(n_samples, n_features),
        index=[f'Sample_{i}' for i in range(n_samples)],
        columns=[f'Gene_{i}' for i in range(n_features)]
    )
    
    # æ¨¡æ‹Ÿè›‹ç™½ç»„æ•°æ®
    proteomics = pd.DataFrame(
        np.random.randn(n_samples, n_features),
        index=[f'Sample_{i}' for i in range(n_samples)],
        columns=[f'Protein_{i}' for i in range(n_features)]
    )
    
    # æ¨¡æ‹Ÿä»£è°¢ç»„æ•°æ®
    metabolomics = pd.DataFrame(
        np.random.randn(n_samples, n_features),
        index=[f'Sample_{i}' for i in range(n_samples)],
        columns=[f'Metabolite_{i}' for i in range(n_features)]
    )
    
    # åˆ›å»ºå¤šç»„å­¦æ•´åˆå™¨
    integrator = MultiOmicsIntegrator(name="THSWD_MultiOmics")
    
    # åŠ è½½æ•°æ®
    integrator.load_omics_data('transcriptomics', transcriptomics)
    integrator.load_omics_data('proteomics', proteomics)
    integrator.load_omics_data('metabolomics', metabolomics)
    
    # æ ‡å‡†åŒ–æ•°æ®
    integrator.normalize_data(method='standard')
    
    # æ•´åˆå¤šç»„å­¦æ•°æ®
    integrated_data = integrator.integrate_multiomics(method='pca')
    
    # è®¡ç®—è·¨ç»„å­¦ç›¸å…³æ€§
    correlation = integrator.compute_cross_omics_correlation(method='pearson')
    print("\nè·¨ç»„å­¦ç›¸å…³æ€§:")
    print(correlation)
    
    # æ„å»ºæ•´åˆç½‘ç»œ
    network = integrator.build_integration_network(threshold=0.3)
    
    # è¯†åˆ«æ¢çº½ç‰¹å¾
    hub_features = integrator.identify_hub_features(top_n=10)
    print("\næ¢çº½ç‰¹å¾:")
    print(hub_features)
    
    # å¯è§†åŒ–æ•´åˆç»“æœ
    integrator.visualize_integration(output_dir='./results/multi_omics_integration')
    
    print("\n" + "=" * 60)
    print("âœ… å¤šç»„å­¦æ•´åˆæ¡†æ¶æ¼”ç¤ºå®Œæˆï¼")
    print("=" * 60)


if __name__ == '__main__':
    demo_multi_omics_integration()